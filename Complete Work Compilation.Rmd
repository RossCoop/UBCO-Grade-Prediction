---
title: "Complete Work"
author: "Ross Cooper 54907605"
date: "2024-07-10"
output: html_document
---


# Loading Packages

```{r loading all libraries used, eval=TRUE, echo=FALSE}
# does not necessarily include library calls in chunks not used in knitting process
library(dplyr)
library(knitr)
library(missForest)
library(randomForest)
library(gbm)
library(recosystem)
library(e1071)
library(kernlab)
library(caret)
library(tidyverse)
library(ggplot2)
```


# Data Cleaning and Processing

## Student grades data cleaning

The first thing we do is read in the student grades from the csv given. After that we trim the whitespace for all character columns to ensure there are no irregularities. A course code column is created to put a course with it's course number in the form COURSE.NUMBER.
```{r reading in student-data, eval = FALSE, echo=FALSE}
## this chunk is used to read in student grades data from student-data.csv
## and cleans the data to be used
# df <- read.csv("..\\UBCO-Grade-Prediction-data\\student-data.csv")

# df_clean <- df[,c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18)]

# loading new data
load(file="new_merged_data.RData")

df_clean <- final_data

# Trim whitespace for all character columns
df_clean$STUD_NO_ANONYMOUS <- trimws(df_clean$STUD_NO_ANONYMOUS)
df_clean$CRS_DPT_CD <- trimws(df_clean$CRS_DPT_CD)
# df_clean$HDR_CRS_LTTR_GRD <- trimws(df_clean$HDR_CRS_LTTR_GRD)  not in the new data
df_clean$CURR_SPEC_PRIM_PGM_TYPE_1 <- trimws(df_clean$CURR_SPEC_PRIM_PGM_TYPE_1)
df_clean$CURR_SPEC_PRIM_SUBJECT_1 <- trimws(df_clean$CURR_SPEC_PRIM_SUBJECT_1)
df_clean$CURR_SPEC_PRIM_PGM_TYPE_2 <- trimws(df_clean$CURR_SPEC_PRIM_PGM_TYPE_2)
df_clean$CURR_SPEC_PRIM_SUBJECT_2 <- trimws(df_clean$CURR_SPEC_PRIM_SUBJECT_2)
df_clean$CURR_SPEC_SECN_PGM_TYPE_1 <- trimws(df_clean$CURR_SPEC_SECN_PGM_TYPE_1)
df_clean$CURR_SPEC_SECN_SUBJECT_1 <- trimws(df_clean$CURR_SPEC_SECN_SUBJECT_1)
df_clean$CURR_SPEC_SECN_PGM_TYPE_2 <- trimws(df_clean$CURR_SPEC_SECN_PGM_TYPE_2)
df_clean$CURR_SPEC_SECN_SUBJECT_2 <- trimws(df_clean$CURR_SPEC_SECN_SUBJECT_2)
df_clean$DEGR_PGM_CD <- trimws(df_clean$DEGR_PGM_CD)
df_clean$SEC_SES_YR <- trimws(df_clean$SEC_SES_YR)
df_clean$SEC_NO <- trimws(df_clean$SEC_NO)
df_clean$SEC_SES_CD <- trimws(df_clean$SEC_SES_CD)
df_clean$GENDER <- trimws(df_clean$GENDER)
df_clean$CITIZENSHIP <- trimws(df_clean$CITIZENSHIP)
df_clean$RESIDENCY <- trimws(df_clean$RESIDENCY)
df_clean$CRS_DTL_CD <- trimws(df_clean$CRS_DTL_CD)

# Factor grades column
# grades <-
#   c("A+", "A", "A-", "B+", "B", "B-", "C+", "C", "C-", "D", "F")
# df_clean$HDR_CRS_LTTR_GRD <-
#   factor(df_clean$HDR_CRS_LTTR_GRD, levels = grades)

# Create course code column
df_clean$COURSE_CODE <- paste0(paste(df_clean$CRS_DPT_CD, df_clean$CRS_NO, sep = "."), df_clean$CRS_DTL_CD)
df_clean <- df_clean[,-(12:13)]
df_clean <- df_clean[, -(20)]

# Changing course detail, gender, res, and citizenship from "" to NA

df_clean <- df_clean %>%
  mutate(RESIDENCY = na_if(RESIDENCY, ""))
df_clean <- df_clean %>%
  mutate(CITIZENSHIP = na_if(CITIZENSHIP, ""))
df_clean <- df_clean %>%
  mutate(GENDER = na_if(GENDER, ""))


# Removing all withdrawl grades
df_clean <- df_clean[df_clean$HDR_CRS_PCT_GRD < 999,]

# Changing all withdrawls (999.9) to -1 to ensure they're kept in the data
#df_clean$HDR_CRS_PCT_GRD[df_clean$HDR_CRS_PCT_GRD == 999.9] <- -1

# Create year code column
df_clean$YEAR_CODE <- paste0(df_clean$SEC_SES_YR, df_clean$SEC_SES_CD)
df_clean <- df_clean[,-(14:15)]

# Removing 2023 grades (not included) [also does nothing bc ^ cleans it out]
#df_clean <- subset(df_clean, SEC_SES_YR < 2022.5)


# for if we want only BSc students (better to keep all data in)
#df_bsc <- df_clean[df_clean$DEGR_PGM_CD=="BSC-O",]


head(df_clean)



# save(df_clean, file = "student-data.RData")
```

The student grades data has not fundamentally changed.
```{r loading cleaned student-data, eval=TRUE, echo=FALSE}
load("student-data.RData")
kable(df_clean[c(1:5),c(1,3,4,5,13,18,19)])
```
c(1,2,3,4,5,12,13,14,18)
This chunks' main purpose is to make a table summing the number of times each course is taken. This also identifies the "Relevant Courses" as df_bsc_relevant_courses. Outputs the number of unique courses found after cleaning the data.
```{r finding number of times courses were taken, eval=TRUE, echo=FALSE}
# Determining how many courses there are (taken by at least 1 student)
unique(df_clean$COURSE_CODE)|>length()

# making a table indicating the number of times each course was taken
courses_taken_bsc <- table(df_clean$COURSE_CODE)

# previous lines of code
# sorted_courses_taken_bsc <- sort(courses_taken_bsc, decreasing = TRUE)[1:200]
sorted_courses_taken_bsc <- sort(courses_taken_bsc, decreasing = TRUE)
# sorted_courses_taken_bsc <- sorted_courses_taken_bsc[substr(names(sorted_courses_taken_bsc),1,4) %in% c("DATA","MATH",
                                                   # "COSC","STAT")]

# current line which removes any courses not taken at least 10 times
# sorted_courses_taken_bsc <- sort(courses_taken_bsc[courses_taken_bsc >= 10], decreasing = TRUE)

# barplot showing # of times a course is taken
# barplot(sorted_courses_taken_bsc)
# saving the course names of those which remain
course_names <- dimnames(sorted_courses_taken_bsc)[[1]]

# Subsetting on only courses which are taken 10 or more times ("relevant courses")
df_bsc_relevant_courses <- df_clean[df_clean$COURSE_CODE %in% course_names, ]

# Getting a list of all student ids which took relevant courses
uniqueids <- unique(df_bsc_relevant_courses$STUD_NO_ANONYMOUS)
```

This chunk was used to determine the maximum number of times each course was taken. With the number of times a course was taken, we can make enough columns for each students' retaken course grade without issue.
```{r counting course retake occurences, eval=FALSE, echo=FALSE}
# finding student who've taken a course more than once
duplicate_rows <- df_bsc_relevant_courses[duplicated(df_bsc_relevant_courses[c("STUD_NO_ANONYMOUS", "COURSE_CODE")]) | duplicated(df_bsc_relevant_courses[c("STUD_NO_ANONYMOUS", "COURSE_CODE")], fromLast = TRUE), ]
# duplicate_rows[c(1,15,12,13)]

# Count occurrences of each course for each student
course_counts <- table(duplicate_rows$STUD_NO_ANONYMOUS, duplicate_rows$COURSE_CODE)

# Find the maximum number of times each course was taken by any student
max_repeats <- apply(course_counts, 2, max)

# Combine course names with corresponding maximum counts
max_course_counts_df <- data.frame(COURSE_CODE = names(max_repeats), max_repeats)
```

Here we create those columns header names to be able to create a empty data frame to fill.
```{r creating list of course column names .1 .2, eval=FALSE, echo=FALSE}
# Creating a new list of course column names
## eg. Math.101.1, Math.101.2...

## want a list of course names with the .1 .2 .3
# groups: Non-Repeats, Repeats
# Non-repeats
non_rep_courses <- setdiff(course_names,max_course_counts_df$COURSE_CODE)

# Repeats
# Modify course names for repeated courses
rep_courses <- c()
for (i in 1:nrow(max_course_counts_df)) {
  for (j in 1:max_course_counts_df[i,2]) {
    rep_courses <- append(rep_courses, paste0(max_course_counts_df[i,1], ".", j))
  }
}

course_names_dups <- append(non_rep_courses, rep_courses)
```


### Extracting student majors and minors
This chunk is used to find each students' major(s), minor, and whether they are taking honors. They are saved in a data frame with each student on a new row. This data frame is later fed into the studentgrades data frame including all student grades. This chunk will be run for any of the three types of students grade structure used.
```{r finding student majors and minors, eval=FALSE, echo=FALSE}
# Finding student majors and minors
### WARNING: no 2023 data yet which may contain updates to major/minors


# Assuming your dataframe is called 'df'

# Order all rows in df by student year
last_rows <- df_bsc_relevant_courses[order(df_bsc_relevant_courses$CURR_YEAR_LEVEL), ]

# Group by student_id and get the last row of each group
last_rows <- df_bsc_relevant_courses %>%
  group_by(STUD_NO_ANONYMOUS) %>%
  slice(n())


# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(last_rows), ncol = 4))


# Add course name columns to existing data frame
student_mm <- cbind(last_rows[,1], new_columns)
colnames(student_mm) <- c("StudentID", "Major.1", "Major.2", "Minor", "Honors")
student_mm$Honors <- FALSE

for(i in 1:nrow(last_rows)){
  # If MAJ is in column 1, major.1 is that subject
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="MAJ"){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
  }
  
  # if MAJ is in col 2, check if major.1 is occupied and place it in maj.1 or maj.2
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="MAJ" & is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="MAJ"){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  }
  
  # combined majors....
  
  # finding that minor
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="MIN"){
    student_mm[i,]$Minor <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="MIN"){
    student_mm[i,]$Minor <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  }
  
  # adding Honors as majors
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="HON" & is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
    student_mm[i,]$Honors <- TRUE
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="HON" & is.na(student_mm[i,]$Major.2)){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
    student_mm[i,]$Honors <- TRUE
  }
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="HON" & is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
    student_mm[i,]$Honors <- TRUE
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="HON" & is.na(student_mm[i,]$Major.2)){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
    student_mm[i,]$Honors <- TRUE
  }
  
  # adding combined majors
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="CMJ"){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_SECN_SUBJECT_1
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="CMJ" & !is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  }
  
}
head(student_mm)
```

### Extracting gender, citizenship, and residency
```{r extracting gender, citizenship, and residency from new student rows, eval=FALSE, echo=FALSE}

# getting only rows with values for the gender, etc. columns
rel_rows <- df_bsc_relevant_courses[!is.na(df_bsc_relevant_courses$GENDER) & !is.na(df_bsc_relevant_courses$RESIDENCY) & !is.na(df_bsc_relevant_courses$CITIZENSHIP),]


# Group by student_id and get any row each group (since we assume gender, citizenship, nor residency changes)
last_rows <- rel_rows %>%
  group_by(STUD_NO_ANONYMOUS) %>%
  slice(n())

# Students' gender, citizenship, and residency
student_gcr <- last_rows[,c(1,15,16,17)]
colnames(student_gcr) <- c("StudentID", "Gender", "Citizenship", "Residency")

# Join onto student_mm
student_details <- left_join(student_mm, student_gcr, by = "StudentID")
```



# Data Structure
## Using all student grades from retaken courses

This variation of studentgrades has each student represented as rows, and courses as columns. Course grades are recorded for each attempt a student takes at each course. Each new attempt is saved in a new column with all 3rd time attemps of MATH.101 being saved in MATH.101.3. For example, MATH 101 has 3 columns: MATH.101.1, MATH.101.2, MATH.101.3. One for each new attempt at MATH 101.

Here we create an empty data frame, add in the unique student ids and their matching majors and minors. After we loop through df_bsc_relevant_courses to add in grades to their respective student and course column.
```{r Making studentgrades with .1 .2 etc, eval=FALSE, echo=FALSE}
# Making the Data Frame
 ## Columns to add: Student ID, Major, Honors, Minor, Extras, 200 most common courses

# Building df after getting the data

studentgrades <- data.frame(Student_ID = sort(uniqueids))


# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(studentgrades), ncol = length(course_names_dups)))
colnames(new_columns) <- course_names_dups

# Add course name columns to existing data frame
studentgrades <- cbind(studentgrades, student_mm[,-1], new_columns)


# Saving student grades into df
for (i in 1:nrow(df_bsc_relevant_courses)){
  rep <- 0
  # finding the index of course name .1
  coursecol <- grep(df_bsc_relevant_courses[i,]$COURSE_CODE, colnames(studentgrades))[1]
  
  # Moving over from .1 to .2 columns if a grade already exists
  while(!is.na(studentgrades[studentgrades$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+rep])){
    rep = rep+1
  }
  
  # Saving student grade into studentgrades df
  studentgrades[studentgrades$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+rep] <- df_bsc_relevant_courses[i,]$HDR_CRS_PCT_GRD
}

studentgrades[,c("Student_ID","MATH.101.1","MATH.101.2")]


```

#### Loading studentgrades_rep

```{r loading studentgrades with .1 .2, eval=TRUE, echo=FALSE}
load(file = "studentgrades_1.RData")
kable(studentgrades_rep[c(1:5),c(1:5,810:815)])
```



## Using only the most recent course grades

This variation of studentgrades has each student represented as rows, and courses as columns. Only the most recent attempt at each course is recorded. For example, MATH 101 has only one column: MATH.101.

To do that we only create one column per course and save the most recent grade into correct student row and course column. This will overwrite existing grades if a student has retaken a course.
```{r Making studentgrades using most recent grade, eval=FALSE, echo=FALSE}
studentgrades <- data.frame(Student_ID = sort(uniqueids))

# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(studentgrades), ncol = length(course_names)))
colnames(new_columns) <- course_names

# Add course name columns to existing data frame
studentgrades <- cbind(studentgrades, student_details[,-1], new_columns)


# Saving student grades into df
for (i in 1:nrow(df_bsc_relevant_courses)){
  
  # finding the index of course name .1
  coursecol <- grep(df_bsc_relevant_courses[i,]$COURSE_CODE, colnames(studentgrades))[1]
  
  # Saving student grade into studentgrades df
  studentgrades[studentgrades$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol] <- df_bsc_relevant_courses[i,]$HDR_CRS_PCT_GRD
}

studentgrades
# save(studentgrades, file="StudentGrades.RData")
```

#### Loading studentgrades_new

```{r loading studentgrades with new student data, eval=TRUE, echo=FALSE}
load("StudentGrades.RData")
studentgrades_new = studentgrades
kable(studentgrades_new[c(1:5),c(1:10)])
```

#### Loading studentgrades

```{r loading studentgrades using most recent grade, eval=TRUE, echo=FALSE}
load("StudentGrades_old.RData")
kable(studentgrades[c(1:5),c(1:10)])
```



## Using Professor data
### Cleaning process
Since all professors for each course are saved in individual csvs by subject and term we need to compile all professor data into one data frame. We save the resulting into a rdata file.
```{r compiling prof csv data, eval=FALSE, echo=FALSE}
# getting all file names in UBCO-Profs folder
file_names <- dir(path = "UBCO-Profs")

# full prof dataframe
course_profs <- data.frame(Professor=character(), year_code=character(), course_name=character(), Section=character())

for(i in 1:length(file_names)){
  # Try to open the file connection
  file_connection <- try(read.csv(paste0("UBCO-Profs/", file_names[i])), silent = TRUE)

  # Check if the connection was successful
  if (inherits(file_connection, "try-error")) {
    
  } else {
    df <- file_connection
    df$course_name <- paste(file_connection$Subject, file_connection$Course, sep = ".")
    df$year_code <- paste0(file_connection$Year, file_connection$Session)
    df <- df[,c("Professor", "year_code", "course_name", "Section")]
    
    course_profs <- rbind(course_profs, df)
  }

}

# both of these have no prof names
course_profs <- course_profs[course_profs$Section != "OVERALL",]
course_profs <- course_profs[course_profs$Professor != "",]

save(course_profs, file = "course_profs.RData")
```

However, this data is messy and has several names in the professor column. My assumption is that TAs and co professors are added to the list along with the main professor. This chunk is used in the cleaning process and finds out how many times each unique professor name appears in the data frame.
```{r adjusting prof data table, eval=FALSE, echo=FALSE}
# all unique 'professor' names
prof_names_full_data <- course_profs$Professor
# length
len <- length(prof_names_full_data)

# empty vectors
prof_names_all <- character()
prof_lens <- numeric()

# looping through to fill two vectors
for(i in 1:len){
  # adding all names to a pile of names (yes duplicates)
  prof_names_all <-  c(prof_names_all, strsplit(prof_names_full_data[i], ";")[[1]])
  # adding each unique 'professor' name's length (ie. # of profs)
  prof_lens <- c(prof_lens, length(strsplit(prof_names_full_data[i], ";")[[1]]))
}

# DF of all 'prof' names with more than one person
# course_profs[which(prof_lens > 1),]


# creating a table count of # of times profs taught
prof_table <- table(prof_names_all)
ordered_profs <- sort(prof_table, decreasing = TRUE)
```


We need to reduce the number of names in each column to only a single name in order to standardize and factor professors. While rudimentary, the one name we take is the name which has appeared the most times in the entire data frame. The logic behind this is that the names with the most occurrences over the years will be professors as opposed to students who TA for 2 years. We don't count only names which are by themselves (no TAs etc.) to find professors with for one reason. Many subjects only list professors with their TAs resulting in many professors being excluded from the table which counts name occurrences. This messes with the professor of many courses as no professor is found in the name occurrences table.

Yes there are better solutions, however as a test to see if professor data would improve the model, this is the algorithm I chose. We could change this to save multiple professors if both appear on their own. One problem I foresee with this is when the main prof never appears on their own, but the co prof does resulting in only the co professor being saved.
```{r making table of courses and their professors, eval=FALSE, echo=FALSE}
# making a new table to join on (to student grades)
rel_course_profs <- course_profs
ordered_prof_names <- rownames(ordered_profs)


# loop through all rows
for(r in 1:nrow(rel_course_profs)){
  
  # if professor has more than one prof
  if(length(strsplit(rel_course_profs[r,"Professor"],";")[[1]]) > 1){
    
    # loop through prof names to save first one which appears
    for(i in 1:length(ordered_prof_names)){
      
      # checking if current ordered prof is in list of profs
      if(ordered_prof_names[i] %in% strsplit(rel_course_profs[r,"Professor"],";")[[1]]){
          rel_course_profs[r,"Professor"] <- ordered_prof_names[i]
          break
      }
      
    }
  }
    
}


save(rel_course_profs, file = "cleaned_profs.RData")
```

```{r loading professor cleaning and final data, eval=TRUE, echo=FALSE}
# course_profs
load("course_profs.RData")
# rel_course_profs
load("cleaned_profs.RData")
```

The data frame rel_course_profs contains rows containing the Professor, Year, Course name, and Section for each course and section from 2018S to 2023S.


### Professor-student data structure
This was a temporary chunk used to create a data frame to test the model with. courses was saved to a short list of 8 courses which was used to initially test with GBMs
```{r adding profs to studentgrades *OLD*, eval=FALSE, echo=FALSE}
courses <- unique(rel_course_profs$course_name)
courses <- colnames(studentgrades[,colnames(studentgrades) %in% courses])

courseprof_cols <- c(courses, paste0(courses,".P"))


# cbind(studentgrades[,c(1:5)],studentgrades[,colnames(studentgrades) %in% courses])

# cutting out all courses which will cause prof error
df_bsc_relevant_courses <- df_bsc[df_bsc$COURSE_CODE %in% courses,]

# adding in professors
matching_profs <- character()
for(i in 1:nrow(df_bsc_relevant_courses)){
  if(length(rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor) == 0){
    matching_profs <- c(matching_profs, NA)
    # next
    
  } else {
    matching_profs <- c(matching_profs, rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor)
  }
}

rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor


length(matching_profs)
nrow(df_bsc_relevant_courses)

# creating list of unique ids
uniqueids <- unique(df_bsc_relevant_courses$STUD_NO_ANONYMOUS)

### from another chunk
studentgrades_prof <- data.frame(Student_ID = sort(uniqueids))

# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(studentgrades_prof), ncol = length(courseprof_cols)))
colnames(new_columns) <- courseprof_cols

# Add course name columns to existing data frame
studentgrades_prof <- cbind(studentgrades_prof, student_mm[,-1], new_columns)


prof_column_adjust <- length(courses)

# Saving student grades into df
for (i in 1:nrow(df_bsc_relevant_courses)){
  
  ## we have two 36,000 length things, df_bsc_rel_courses and matching_profs
  ## need to pull grades from df and profs from match_profs ROW BY ROW
  ## add to studengrades_prof, can split using + length(courses) between course and prof columns
  
  # finding the index of course name .1
  coursecol <- grep(df_bsc_relevant_courses[i,]$COURSE_CODE, colnames(studentgrades_prof))[1]
  
  # Saving student grade into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol] <- df_bsc_relevant_courses[i,]$HDR_CRS_PCT_GRD
  
  # saving prof name into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+prof_column_adjust] <- matching_profs[i]
}

studentgrades_prof[,880:890]
```

This chunk is used to make a new studentgrades_prof data frame with both courses and their respective professors as columns. By looping through each row in df_bsc_relevant_columns, the relevant data is added to studentgrades_prof.

The data frame studentgrades_prof is similar to studentgrades (most recent grades version) in that students grades are saved as rows, and each columns is a different course. Additionally, there are columns which contain the professors who taught each course there is a grade for. For example, Student X has a grade in the MATH.101 column and will therefore have a professor in the MATH.101.P column. Student Y who has not taken MATH 101 will not have a grade in MATH.101, nor a professor in MATH.101.P.
```{r creating studentgrades_prof, eval=FALSE, echo=FALSE}
# creating all columns to be used in studentgrades_prof
# course_names from 'finding number of times courses were taken' chunk
courseprof_cols <- c(course_names, paste0(course_names,".P"))


# creating list of unique ids
uniqueids <- unique(df_bsc_relevant_courses$STUD_NO_ANONYMOUS)

### from another chunk
studentgrades_prof <- data.frame(Student_ID = sort(uniqueids))

# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(studentgrades_prof), ncol = length(courseprof_cols)))
colnames(new_columns) <- courseprof_cols

# Add course name columns to existing data frame
studentgrades_prof <- cbind(studentgrades_prof, student_details[,-1], new_columns)


prof_column_adjust <- length(course_names)

# Saving student grades into df
for (i in 1:nrow(df_bsc_relevant_courses)){
  
  
  # finding the index of course name
  coursecol <- grep(df_bsc_relevant_courses[i,]$COURSE_CODE, colnames(studentgrades_prof))[1]
  
  # Saving student grade into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol] <- df_bsc_relevant_courses[i,]$HDR_CRS_PCT_GRD
  
  # catching if there is no prof for course
  if(length(rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor) == 1){
    
    
    # saving prof name into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+prof_column_adjust] <- rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor
  }
  
  # catching if there are more than one prof showed (PSYO 380 since it's missing 'Detail' in main student grades csv)
  if(length(rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor) > 1){
    
    
    # saving prof name into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+prof_column_adjust] <- rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor[1]
  }
  
  
}

studentgrades_prof


# removing columns with no or one prof
cols_to_remove <- numeric(0)
for(i in (prof_column_adjust+6):ncol(studentgrades_prof)){
  if(length(unique(studentgrades_prof[,i])) <= 2){
    cols_to_remove <- c(cols_to_remove, i)
  }
}

studentgrades_prof <- studentgrades_prof[,-cols_to_remove]


# factoring professor columns
for(i in (prof_column_adjust+6):ncol(studentgrades_prof)){
  studentgrades_prof[,i] <- factor(studentgrades_prof[,i])
}

save(studentgrades_prof, file="Prof_StudentGrades.RData")
```

#### Loading studentgrades_prof_new

```{r loading studentgrades_prof using new student data, eval=TRUE, echo=FALSE}
load(file="Prof_StudentGrades.RData")
studentgrades_prof_new = studentgrades_prof
kable(studentgrades_prof_new[1:5,1:12])
```

#### Loading studentgrades_prof

This is the original student grades with professor data. And what I will be using for the most part unless specified.
```{r loading studentgrades_prof, eval=TRUE, echo=FALSE}
load(file="Prof_StudentGrades_old.RData")
kable(studentgrades_prof[1:5,c(1:7,910:912)])
```

#### Loading Professor Course List

Table of courses and the professors which taught them.
```{r showing prof list, eval=TRUE, echo=FALSE}
kable(rel_course_profs[1:10,])
```


### Adding professors to the original dataset
```{r creating original dataset with professors, eval=FALSE, echo=FALSE}
df_clean$Professor = character(nrow(df_clean))

for(i in 1:nrow(df_clean)){
  for(j in 1:nrow(rel_course_profs)){
    if((df_clean$YEAR_CODE[i] == rel_course_profs$year_code[j]) & (df_clean$COURSE_CODE[i] == rel_course_profs$course_name[j]) & (df_clean$SEC_NO[i] == rel_course_profs$Section[j])){
      df_clean$Professor[i] = rel_course_profs$Professor[j]
    }
  }
  
}

df_clean

write.csv(df_clean, file = "../UBCO-Grade-Prediction-data/student-data-profs.csv")
```


```{r Making factored dataset, eval=FALSE, echo=FALSE}
df <- read.csv("../UBCO-Grade-Prediction-data/student-data-profs.csv")
df <- df[,-1]
df <- na.omit(df)


# Convert each column to factors and create reference tables
factor_columns <- lapply(df, function(column) {
  if(is.character(column)) {
    factor_column <- factor(column)
    reference_table <- data.frame(
      original_value = levels(factor_column),
      factor_number = seq_along(levels(factor_column))
    )
    print(reference_table) # Display the reference table
    return(as.integer(factor_column))
  } else {
    return(column)
  }
})

# Combine the factored columns into a new data frame
df_factored <- as.data.frame(factor_columns)

df_factored <- df_factored[order(df_factored$YEAR_CODE),]

# Output the new factored dataframe
print(df_factored)

# write.csv(df_factored, file = "../UBCO-Grade-Prediction-data/student-data-profs-factored.csv")
```

```{r showing factored dataset, eval=FALSE, echo=FALSE}
# courses_factored <- numeric(0)
# courses_factored$num <- seq_along(levels(factor(df$COURSE_CODE)))
# courses_factored$course <- levels(factor(df$COURSE_CODE))
# courses_factored <- data.frame(courses_factored)

courses_factored[courses_factored$course == "STAT.230",]
```


# Analysis and Prediction

Throughout my time on this project I have switched which data frame I used to train and test each model and method. The three data frames are described above but are called studentgrades_rep, studentgrades, and studentgrades_prof respectively. For some of the main methods using 'old' data frames I will use other data frames for comparison.

Here I set up a data frame to save all model rmses
```{r setting up rmse dataframe, eval=TRUE, echo=FALSE}
all_model_rmse <- data.frame(rmse = numeric(0), detail = character(0))
```


## Initial EDA

These are the first plots I made in order to get a feel for the data. They do not show much insight into the data.
```{r plot of MATH.100.1 against MATH.100.2,eval=TRUE, echo=FALSE}
# Big ol graph of student grades
maths <- studentgrades_rep[complete.cases(studentgrades_rep$MATH.100.1, studentgrades_rep$MATH.100.2), ]


maths.lm <- lm(maths$MATH.100.1~ maths$MATH.100.2)
summary(maths.lm)

plot(maths$MATH.100.1, maths$MATH.100.2)
abline(maths.lm)

## weird....
```

```{r plot of MATH.100.1 against STAT.230.1, eval=TRUE, echo=FALSE}
mathstat <- studentgrades_rep[complete.cases(studentgrades_rep$MATH.100.1, studentgrades_rep$STAT.230.1), ]

mathstat.lm <- lm(mathstat$MATH.100.1~ mathstat$STAT.230.1)
summary(mathstat.lm)

plot(mathstat$MATH.100.1, mathstat$STAT.230.1)
abline(mathstat.lm)

## this one is a little more normal
## could be issues with course retakers

## want to check on people who only had to take it once...
```

```{r plot of MATH.100.1 against PSYO.111.1, eval=FALSE, echo=FALSE}
edasubset <- studentgrades_rep[complete.cases(studentgrades_rep$MATH.100.1, studentgrades_rep$PSYO.111.1), ]

edasubset.lm <- lm(edasubset$MATH.100.1~ edasubset$PSYO.111.1)
summary(edasubset.lm)

plot(edasubset$MATH.100.1, edasubset$PSYO.111.1)
abline(edasubset.lm)

## ... makes sense that some courses like PSYO 111 are grade boosters
```

```{r plot of MATH.100.2 against STAT.230.1, eval=TRUE, echo=FALSE}
mathstat2 <- studentgrades_rep[complete.cases(studentgrades_rep$MATH.100.2, studentgrades_rep$STAT.230.1), ]

mathstat2.lm <- lm(mathstat2$MATH.100.2~ mathstat2$STAT.230.1)
summary(mathstat2.lm)

plot(mathstat2$MATH.100.2, mathstat2$STAT.230.1)
abline(mathstat2.lm)
```

```{r plot of MATH.100.1 (no MATH.100.2 grade present) againt STAT.230.1, eval=TRUE, echo=FALSE}

## want to check on people who only had to take it once (aka no MATH.100.2 grade present)

firsttimer <- suppressMessages(anti_join(mathstat,mathstat2))
first.lm <- lm(firsttimer$MATH.100.1~ firsttimer$STAT.230.1)
summary(first.lm)

plot(firsttimer$MATH.100.1, firsttimer$STAT.230.1, xlim = c(0,100))
abline(first.lm)

# just removed the 'low' grades (p much all fails) 
```

This chunk uses studentgrades_rep and makes a data frame including each course and its average grade.
```{r finding average grades for each course, eval=FALSE, echo=FALSE}
# want the average grade for each course... bar plot
course_averages <- data.frame(Courses = course_names_dups, Average = rep(NA, length(course_names_dups)))

for (i in 1:nrow(course_averages)){
  course_averages[i,]$Average <- mean(studentgrades_rep[,course_averages[i,1]],na.rm = TRUE)
}

# barplot of all course average grades
barplot(course_averages$Average, names.arg = course_averages$Courses)

# average grades for each course
head(course_averages)

## doesnt work atm.
```

This chunk uses studentgrades_rep to make a pairplot of MATH.100.1, MATH.101.1, STAT.230.1, DATA.101.1.
```{r pairplot of MATH.100.1, MATH.101.1, STAT.230.1, DATA.101.1, eval=TRUE, echo=FALSE}
# pairplot using data which has no missing grades from these courses
plot(studentgrades_rep[!is.na(studentgrades_rep$MATH.100.1)&!is.na(studentgrades_rep$MATH.101.1)&!is.na(studentgrades_rep$STAT.230.1)&!is.na(studentgrades_rep$DATA.101.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","DATA.101.1")])
```

This is the same pairplot as above using only students who have taken STAT 230.
```{r, echo=FALSE, eval=TRUE}
# studentgrades data which has no missing STAT.230 grades
# studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")]

# pairplot using data which has no missing STAT.230 grades
plot(studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")])
```

```{r plot of MATH.100.1 against MATH.101.1, eval=TRUE, echo=FALSE}
plot(studentgrades_rep[!is.na(studentgrades_rep$MATH.100.1)&!is.na(studentgrades_rep$MATH.101.1),c("MATH.100.1","MATH.101.1")])
```

This chunk returns the number of times a course has been taken by students WHO HAVE ALL TAKEN stat 230. Used to view variables which may be useful. This code is used in a later chunk.
```{r number of times courses have been taken by those who have taked STAT 230, eval=FALSE, echo=FALSE}
# returns the number of times a course has been taken by students WHO HAVE ALL TAKEN stat 230
ids <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$COURSE_CODE=="STAT.230",]$STUD_NO_ANONYMOUS)
courses <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids,]$COURSE_CODE)
new_courses <- character(0)
for(i in 1:length(courses)){
  if(strsplit(courses[i], split = ".", fixed = TRUE)[[1]][2] < 300){
    new_courses <- append(new_courses, courses[i])
  }
}

courses_taken_bsc_stat230 <- table(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids & df_bsc_relevant_courses$COURSE_CODE %in% new_courses,]$COURSE_CODE)


sorted_courses_taken_bsc <- sort(courses_taken_bsc_stat230, decreasing = TRUE)
sorted_courses_taken_bsc

# number of courses who have been taken by at least N people: 25 people = 105 courses | 250 people = 25 courses
N = 25
length(sorted_courses_taken_bsc[sorted_courses_taken_bsc >= N])


```

This is an oversimplified linear regression on STAT.230 using pre requisite courses. Oversimplified meaning we take COSC.221 and MATH.101 as the pre reqs when in reality they are: One of MATH 101, MATH 103, MATH 142 and one of DATA 101, COSC 221.
```{r linear regression on stat.230 prerequisites, eval=TRUE, echo=FALSE}
rel_grades <- studentgrades[complete.cases(studentgrades$STAT.230, studentgrades$COSC.221, studentgrades$MATH.101), ]

set.seed(5934)
n_rows <- nrow(rel_grades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

lr_model <- lm(STAT.230 ~ COSC.221+MATH.101, data = rel_grades[train_indices,])
summary(lr_model)

predictions <- predict(lr_model, rel_grades[test_indices,])
actual <- rel_grades[test_indices,"STAT.230"]

rmse <- sqrt(mean((predictions - actual)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Linear Regression on Prerequisites"))


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```


## First Prediction Attempts

These first attempts were made on either studentgrades_rep or studentgrades. Attempts made using studentgrades_prof were added in after for comparison.

### MissForest
The MissForest package is used to impute missing data. After using it to impute NA's into student grades, we can use the imputed data set with other models which don't allow for missing data/NAs. One of this packages use cases was imputing the data and using it in a Random Forest model. Since imputation is computationally expensive, we only impute on a small number of courses. 
Unfortunately, due to the way in which I implemented this model into others there was some leaked information resulting in misleading rmse values. 
This chunk uses studentgrades_rep and only uses each selected courses first attempt to impute and predict.
```{r missForest Impute into RF on course first attempts, eval=FALSE, warning=FALSE, echo=FALSE}
# preparing all student grades with stat230
stat230grades <- studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","MATH.200.1","MATH.221.1","COSC.211.1","COSC.222.1","COSC.221.1")]
head(stat230grades)


n_rows <- nrow(stat230grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,2:9])$ximp

stat230.rf <- randomForest(STAT.230.1~., data=stat230.imputed[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- stat230grades[test_indices,]$STAT.230.1

# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation into Random Forest (on studentgrades_rep)"))
```

This chunk uses studentgrades with the same few selected courses. While this is one very good result I have tested on a few other seeds and it generally performs quite well. With some CV or other methods this could work well.
```{r missForest Impute into RF using few courses (stats maths cosc), eval=FALSE, echo=FALSE}

# preparing all student grades with stat230
stat230grades <- studentgrades[!is.na(studentgrades$STAT.230),c("Student_ID","STAT.230","MATH.100","MATH.101","MATH.200","MATH.221","COSC.211","COSC.222","COSC.221")]
# stat230grades


n_rows <- nrow(stat230grades)

set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing


stat230.imputed <- missForest(stat230grades[,2:9])$ximp

stat230.rf <- randomForest(STAT.230~., data=stat230.imputed[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,-1])

stat230.real <- stat230grades[test_indices,]$STAT.230

# RMSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation into Random Forest (on studentgrades)"))

# graphs
## predicted vs actual plot
predictions <- stat230.pred
actual <- stat230.real


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))
abline(0, 1, col = "red")

line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")


## residuals plot
residuals <- actual - predictions
plot(predictions, residuals, xlab = "Predicted Grades", ylab = "Residuals", 
     main = "Residual Plot", pch = 16, col = "blue")
abline(h = 0, col = "red")

line.lm <- lm(residuals~predictions)
abline(line.lm, col = "green")


## qq plots
qqnorm(residuals, main = "Normal Q-Q Plot of Residuals")
qqline(residuals, col = "red")
```

```{r missForest Impute into RF data leak test, eval=FALSE, echo=FALSE}

# preparing all student grades with stat230
stat230grades <- studentgrades[!is.na(studentgrades$STAT.230),c("Student_ID","STAT.230","MATH.100","MATH.101","MATH.200","MATH.221","COSC.211","COSC.222","COSC.221")]
# stat230grades


n_rows <- nrow(stat230grades)

set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

# imputation section
stat230.imputed.train <- missForest(stat230grades[train_indices,2:9])$ximp
stat230.imputed <- missForest(rbind(stat230.imputed.train[,-1],stat230grades[test_indices,-c(1,2)]))$ximp

# Random Forest section
stat230.rf <- randomForest(STAT.230~., data=stat230.imputed.train)
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- stat230grades[test_indices,]$STAT.230

# RMSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
rmse
# all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation into Random Forest (on studentgrades)"))

# graphs
## predicted vs actual plot
predictions <- stat230.pred
actual <- stat230.real


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))
abline(0, 1, col = "red")

line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")


## residuals plot
residuals <- actual - predictions
plot(predictions, residuals, xlab = "Predicted Grades", ylab = "Residuals", 
     main = "Residual Plot", pch = 16, col = "blue")
abline(h = 0, col = "red")

line.lm <- lm(residuals~predictions)
abline(line.lm, col = "green")


## qq plots
qqnorm(residuals, main = "Normal Q-Q Plot of Residuals")
qqline(residuals, col = "red")
```


In this chunk I wanted to try a method that worked before with the new studentgrades_prof data. As we can see it still works even when imputing professor data.
```{r missForest Impute into RF on studentgrades_prof, eval=FALSE, echo=FALSE}
set.seed(5934)
# preparing all student grades with stat230
stat230grades <- studentgrades_prof[!is.na(studentgrades_prof$STAT.230),c("MATH.100","MATH.101","STAT.230","MATH.200","MATH.221","COSC.211","COSC.222","COSC.221","MATH.100.P","MATH.101.P","STAT.230.P","MATH.200.P","MATH.221.P","COSC.211.P","COSC.222.P","COSC.221.P")]
# stat230grades


n_rows <- nrow(stat230grades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades)$ximp

stat230.rf <- randomForest(STAT.230~., data=stat230.imputed[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- stat230grades[test_indices,]$STAT.230

# RMSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation into Random Forest (on studentgrades_prof"))

# graphs
## predicted vs actual plot
predictions <- stat230.pred
actual <- stat230.real


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))
abline(0, 1, col = "red")

line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")

```


```{r missForest Impute into RF on all studentgrades_prof courses, eval=FALSE, echo=FALSE}

# Identifying the course of interest
COI <- "STAT.230"
coursegrades <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades <- coursegrades[coursegrades$STAT.230 >= 5, ]

# factoring columns
coursegrades$Major.1 <- factor(coursegrades$Major.1)
coursegrades$Minor <- factor(coursegrades$Minor)


# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]

# coursegrades <- coursegrades[!is.na(coursegrades$MATH.101), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]
student_columns <- colnames(coursegrades)[c(1:5)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)[6:length(colnames(coursegrades))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

head(coursegrades)


n_rows <- nrow(coursegrades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(coursegrades)$ximp

stat230.rf <- randomForest(STAT.230~., data=stat230.imputed[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- coursegrades[test_indices,]$STAT.230

# RMSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation into Random Forest using all courses (on studentgrades_prof)"))

# graphs
## predicted vs actual plot
predictions <- stat230.pred
actual <- stat230.real



plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")

```


In this chunk I cut right to the chase and use the imputed values as grade predictions. As expected it doesn't work very well.
```{r Using missForest for prediction, eval=TRUE, echo=FALSE}
set.seed(5934)
# preparing all student grades with stat230
stat230grades <- studentgrades[!is.na(studentgrades$STAT.230),c("MATH.100","MATH.101","STAT.230","MATH.200","MATH.221","COSC.211","COSC.222","COSC.221")]
# stat230grades


n_rows <- nrow(stat230grades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

# to remove stat 230 grades from test_indicies rows
stat230.real <- stat230grades[test_indices,]$STAT.230
stat230grades[test_indices,]$STAT.230 <- NA

stat230.imputed <- missForest(stat230grades)$ximp


stat230.pred <- stat230.imputed[test_indices,]$STAT.230


# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation as prediction (on studentgrades)"))

predictions <- stat230.pred
actual <- stat230.real

plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```


### Random Forest
In this section I've added the other ways I implemented random forests since many attempts were included above since missForest was used in order to keep as many rows as possible.

In this chunk I take the complete cases of MATH.100.1, MATH.101.1, STAT.230.1, and DATA.101.1 to train a RF model and predict STAT 230 grades. Note that this is using studentgrades_rep.
```{r RF on all non-NA data, eval=TRUE, echo=FALSE}
# getting all students with grades in all courses
stat230grades.full <- studentgrades_rep[!is.na(studentgrades_rep$MATH.100.1)&!is.na(studentgrades_rep$MATH.101.1)&!is.na(studentgrades_rep$STAT.230.1)&!is.na(studentgrades_rep$DATA.101.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","DATA.101.1")]


n_rows <- nrow(stat230grades.full)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing


stat230.rf <- randomForest(STAT.230.1~., data=stat230grades.full[train_indices,2:5] )
stat230.pred <- predict(stat230.rf, newdata = stat230grades.full[test_indices,2:5])

stat230.real <- stat230grades.full[test_indices,2:5]$STAT.230.1

# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Random Forest on non-NA data (studentgrades_rep)"))

predictions <- stat230.pred
actual <- stat230.real
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")

## MSE of data without DATA101 is 175...
## looks like imputing data and running a randomforest is best so far
```

This chunk is a repeat of the previous one, except using studentgrades. Neither seem to produce decent predictions, however, I believe there is not enough data to make a good model.
```{r RF on complete data, eval=TRUE, warning=FALSE, echo=FALSE}
# this chunk uses a random forest model trained on the complete set of STAT 230, MATH 101, and COSC.221 (aka no NAs between them)
coursegrades <- studentgrades[!is.na(studentgrades$STAT.230)&!is.na(studentgrades$MATH.101)&!is.na(studentgrades$MATH.100)&!is.na(studentgrades$DATA.101),c("STAT.230","MATH.101","MATH.100","DATA.101")]

set.seed(5934)


n_rows <- nrow(coursegrades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.rf <- randomForest(STAT.230~., data=coursegrades[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = coursegrades[test_indices,])

predictions <- stat230.pred
actual <- coursegrades[test_indices,]$STAT.230

# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - actual)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Random Forest on non-NA data (studentgrades)"))



plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

This chunk implements missForest and Random Forests to predict grades. However, only courses which have been taken a certain number of times (times_taken = 190) are used as predictors during imputation, training, and testing. This is a rudimentary variable selection method but ensures we use predictors with the most existing data. Leaks data unfortunately.
```{r missForest Impute into RF on all courses taken x times, eval=FALSE, echo=FALSE}
# returns the number of times a course has been taken by students who have also taken stat 230
ids <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$COURSE_CODE=="STAT.230",]$STUD_NO_ANONYMOUS)
courses <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids,]$COURSE_CODE)

# this is used to filter out course predictors (columns) which are from years above the course we're predicting on (STAT.230)
new_courses <- character(0)
for(i in 1:length(courses)){
  if(strsplit(courses[i], split = ".", fixed = TRUE)[[1]][2] < 300){
    new_courses <- append(new_courses, courses[i])
  }
}

courses_taken_bsc_stat230 <- table(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids & df_bsc_relevant_courses$COURSE_CODE %in% new_courses,]$COURSE_CODE)


sorted_courses_taken_bsc <- sort(courses_taken_bsc_stat230, decreasing = TRUE)

##
## Filter course names used base on how many people have taken those predictor courses
##
times_taken <- 190
course_names <- sorted_courses_taken_bsc[sorted_courses_taken_bsc >= times_taken]


course_names <- dimnames(course_names)[[1]]


coursegrades <- studentgrades[!is.na(studentgrades$STAT.230),colnames(studentgrades) %in% course_names]
head(coursegrades)


set.seed(5934)
n_rows <- nrow(coursegrades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(coursegrades[,-1])$ximp

stat230.rf <- randomForest(STAT.230~., data=stat230.imputed[train_indices,])
predictions <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- coursegrades[test_indices,]$STAT.230

# RMSE value to compare to other models
rmse <- sqrt(mean((predictions - stat230.real)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation into Random Forest (on courses taken by >300 students)"))
```

This is a repeat of the last chunk with modifications which make it easier to use different courses as the course we predict for.
```{r missForest Impute into RF on all courses *USED FOR TESTING ON DIFF COURSES* (COSC.221), eval=FALSE, echo=FALSE}
# returns the number of times a course has been taken by students who have also taken xxxx.xxx
ids <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$COURSE_CODE=="COSC.221",]$STUD_NO_ANONYMOUS)
courses <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids,]$COURSE_CODE)
new_courses <- character(0)
for(i in 1:length(courses)){
  if(strsplit(courses[i], split = ".", fixed = TRUE)[[1]][2] < 300){
    new_courses <- append(new_courses, courses[i])
  }
}

courses_taken_bsc_stat230 <- table(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids & df_bsc_relevant_courses$COURSE_CODE %in% new_courses,]$COURSE_CODE)


sorted_courses_taken_bsc <- sort(courses_taken_bsc_stat230, decreasing = TRUE)
times_taken <- 50
course_names <- sorted_courses_taken_bsc[sorted_courses_taken_bsc >= times_taken]



set.seed(5934)

# of the same year and below
# preparing all student grades with stat230
course_names <- dimnames(course_names)[[1]]



# course_names <- c("COSC.121","MATH.200","MATH.221","COSC.221","COSC.211","MATH.101","COSC.222","ECON.101","COSC.111","PSYO.111","STAT.230")

stat230grades <- studentgrades[!is.na(studentgrades$COSC.221),colnames(studentgrades) %in% course_names]
head(stat230grades)


n_rows <- nrow(stat230grades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,-1])$ximp

stat230.rf <- randomForest(COSC.221~., data=stat230.imputed[train_indices,])
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- stat230grades[test_indices,]$COSC.221

# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
rmse

```

### Gradient Boosted Machines
This section shows my initial attempts at gradient boosted machines. The first chunk uses studentgrades_rep to train and test on.
```{r GBM on partial-NA data, eval=TRUE, echo=FALSE}
# loading in data again
stat230grades <- studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","COSC.221.1","MATH.101.1","DATA.101.1","STAT.230.1")]
n_rows <- nrow(stat230grades)

set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing


gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 6000,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.01,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230.1 ~ ., data = stat230grades[train_indices,2:5], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new data frame for prediction
predictions <- predict(gbm_model, newdata = stat230grades[test_indices,2:5], n.trees = gbm_params$n.trees)

# Print the predictions
# print(predictions)

rmse <- sqrt(mean((predictions - stat230grades[test_indices,2:5]$STAT.230.1)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Gradient Boosted Machines using 4 coureses as predictors (on studentgrades_rep)"))
## 175 mse... getting worse...

actual <- stat230grades[test_indices,2:5]$STAT.230.1

plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

This chunk uses the complete cases of COSC.221.1, MATH.101.1, STAT.230.1, and DATA.101.1 from studentgrades_rep to train and test. 
```{r GBM on non-NA data, eval=TRUE, echo=FALSE}
# loading in data again TRYING WITH FULL DATA
stat230grades <- studentgrades_rep[!is.na(studentgrades_rep$MATH.100.1)&!is.na(studentgrades_rep$MATH.101.1)&!is.na(studentgrades_rep$STAT.230.1)&!is.na(studentgrades_rep$DATA.101.1),c("Student_ID","COSC.221.1","MATH.101.1","STAT.230.1","DATA.101.1")]
# stat230grades
n_rows <- nrow(stat230grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing


gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.005,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230.1 ~ ., data = stat230grades[train_indices,2:5], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new data frame for prediction
predictions <- predict(gbm_model, newdata = stat230grades[test_indices,2:5], n.trees = gbm_params$n.trees)

# Print the predictions
# print(predictions)

rmse <- sqrt(mean((predictions - stat230grades[test_indices,2:5]$STAT.230.1)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Gradient Boosted Machines on complete cases of 4 courses (on studentgrades_rep)"))
## 128 mse which is a little better when you've got full data


actual <- stat230grades[test_indices,2:5]$STAT.230.1
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

To cover all bases and check if it may be better, we impute grades onto the same courses and train a gbm model. The results are not great but quite a bit better than before.

```{r missForest impute into GBM on studentgrades_rep 4 courses, eval=TRUE, echo=FALSE}
# now, does imputing the data before GBM work better?
# preparing all student grades with stat230
stat230grades <- studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","COSC.221.1","MATH.101.1","DATA.101.1","STAT.230.1")]
# stat230grades


n_rows <- nrow(stat230grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,2:5])$ximp

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.005,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230.1 ~ ., data = stat230.imputed[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = stat230.imputed[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
# print(predictions)

rmse <- sqrt(mean((predictions - stat230.imputed[test_indices,]$STAT.230.1)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation into GBM on 4 courses (studentgrades_rep)"))
## 109 mse.. better still


actual <- stat230.imputed[test_indices,]$STAT.230.1
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

```{r missForest impute into GBM on studentgrades 4 courses, eval=TRUE, echo=FALSE}
# now, does imputing the data before GBM work better?
# preparing all student grades with stat230
stat230grades <- studentgrades[!is.na(studentgrades$STAT.230),c("Student_ID","COSC.221","MATH.101","DATA.101","STAT.230")]
# stat230grades


n_rows <- nrow(stat230grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,2:5])$ximp

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.005,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230 ~ ., data = stat230.imputed[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = stat230.imputed[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
# print(predictions)

rmse <- sqrt(mean((predictions - stat230.imputed[test_indices,]$STAT.230)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation into GBM on 4 courses (studentgrades)"))


actual <- stat230.imputed[test_indices,]$STAT.230
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

```{r missForest impute into GBM on studentgrades all courses, eval=TRUE, echo=FALSE}
# now, does imputing the data before GBM work better?

# Identifying the course of interest
COI <- "STAT.230"
coursegrades <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades <- coursegrades[coursegrades$STAT.230 >= 5, ]

# factoring columns
coursegrades$Major.1 <- factor(coursegrades$Major.1)
coursegrades$Minor <- factor(coursegrades$Minor)


# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]

# coursegrades <- coursegrades[!is.na(coursegrades$MATH.101), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]
student_columns <- colnames(coursegrades)[c(1:5)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)[6:length(colnames(coursegrades))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]


n_rows <- nrow(coursegrades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(coursegrades)$ximp

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.005,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230 ~ ., data = stat230.imputed[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = stat230.imputed[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
# print(predictions)

rmse <- sqrt(mean((predictions - stat230.imputed[test_indices,]$STAT.230)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest imputation into GBM on all courses (studentgrades)"))



actual <- stat230.imputed[test_indices,]$STAT.230
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

This chunk will use students grades of the current year (eg. second year uses STAT.230 and COSC.221) to train the GBM model. However, the test set will have grades from the current year removed (eg. won't use COSC.221 as a predictor). In hindsight this isn't a great idea when using GBMs as the decision trees used won't be configured for the subset of courses used. Also to note, this uses studentgrades_rep.
```{r, eval=FALSE, echo=FALSE}
## sets all courses of equal or higher level to NA before prediction, but after training the model
set.seed(5934)
# time to remove all future year courses from prediction
COI <- "STAT.230.1"
coursegrades <- studentgrades_rep[!is.na(studentgrades_rep[,COI]),-1:-5]

# figuring out the year level of each course...
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- column_names[column_digits <= YOI]
cols_to_keep <- append(cols_to_keep, COI)

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# extras/ to do next
grade_counts <- colSums(!is.na(coursegrades))
# Subset columns with less than 10 grades
sparse_cols <- names(grade_counts[grade_counts <= 5])

# Need to exclude future attempts at the course, but not past
## eg. PSYO.380.1 shouldn't use 380.2 as a predictor, but 380.2 can use 380.1
if(length(strsplit(COI, split = ".", fixed = TRUE)[[1]]) == 3){
  cname <- strsplit(COI, split = ".", fixed = TRUE)[[1]][1]
  cnum <- strsplit(COI, split = ".", fixed = TRUE)[[1]][2]
  crep <- as.integer(strsplit(COI, split = ".", fixed = TRUE)[[1]][3])
  # remove all future instances of course (if they exist)
  for(i in 1:length(column_names)){
    if(strsplit(column_names[i], split = ".", fixed = TRUE)[[1]][1] == cname & strsplit(column_names[i], split = ".", fixed = TRUE)[[1]][2] == cnum & as.integer(strsplit(column_names[i], split = ".", fixed = TRUE)[[1]][3]) > crep){
      sparse_cols <- append(sparse_cols, column_names[i])
    }
  }
}

# Subset the dataframe to include only courses used to predict on
coursegrades <- coursegrades[, -which(names(coursegrades) %in% sparse_cols)]

set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

cols_to_na <- column_names[column_digits == YOI] #### ADD BACK COURSE OF INTEREST!!!!!!!
cols_to_na <- cols_to_na[!cols_to_na == COI]
coursegrades[test_indices, which(names(coursegrades) %in% cols_to_na)] <- NA

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 100,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.1,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)



gbm_model <- gbm(STAT.230.1 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
predictions

rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
rmse

```

### Matrix Factorization

Since the studentgrades data frame is highly dimensional I wanted to see if matrix factorization would work well on this dataset. The results were decent, and no grid search was implemented to further improve this method.
```{r Dataset creation for MF, eval=TRUE, echo=FALSE}
# Getting list of students wanted
studs <- studentgrades[!is.na(studentgrades$STAT.230),]$Student_ID

data <- data.frame(student = character(), course=character(), grade=double())
for(id in studs){
  for(c in 6:205){
    if(!is.na(studentgrades[studentgrades$Student_ID==id,c])){
      data <- rbind(data, list(id,colnames(studentgrades)[c],studentgrades[studentgrades$Student_ID==id,c]))
    }
  }
}

colnames(data) <- c("student","course","grade")

data <- na.omit(data)  # Remove rows with NA ratings

# Split into obj course and not
data_obj <- data[data$course=="STAT.230",]
data_non <- data[!data$course=="STAT.230",]

# Creating the testing set from obj course
n_rows <- nrow(data_obj)

train_indices <- sample(1:n_rows, 0.9 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

data <- rbind(data_obj[test_indices,], data_obj[train_indices,],data_non)


# Factoring data
data[] <- lapply(data, function(x) {
  if (is.character(x)) {
    as.numeric(factor(x))
  } else {
    x
  }
})

obj_course <- data[1,2]
```

```{r Matrix Factorization prediction, eval=TRUE, echo=FALSE}
# Create a Reco object
r <- Reco()

# Define the training set
set.seed(5934)
train_set <- data_memory(user_index = data$student[-1:-length(test_indices)], item_index = data$course[-1:-length(test_indices)], rating = data$grade[-1:-length(test_indices)])

# Train the model with ALS
r$train(train_set, opts = list(dim = 10, costp_l2 = 0.2, costq_l2 = 0.2, lrate = 0.05, niter = 20))


# Initialize the matrix to store predicted grades
predicted_grades <- data[1:length(test_indices),]
predicted_grades$grade <- NA

# Predict missing values

for(i in 1:nrow(predicted_grades)){
  predicted_grades[i,]$grade <- r$predict(data_memory(user_index = predicted_grades[i,]$student, item_index = predicted_grades[i,]$course))
}


# Print the predicted grades matrix
# print(predicted_grades)


rmse <- sqrt(mean((predicted_grades$grade - data_obj[test_indices,]$grade)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Matrix Factorization on the 199 courses taken the most times (studentgrades)"))

predictions <- predicted_grades$grade
actual <- data_obj[test_indices,]$grade
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

### Structured Learning

```{r Structured Learning, eval=FALSE, echo=FALSE}

library(bnlearn)

suppressWarnings({
pc <- pc.stable(studentgrades[,-(1:5)], undirected = FALSE)
pc$arcs # 10 arcs

gs <- gs(studentgrades[,-(1:5)], undirected = FALSE)
gs$arcs # 10 arcs

iamb <- iamb(studentgrades[,-(1:5)])
iamb$arcs # 75 arc... some potential

hc <- tabu(studentgrades[,-(1:5)], score = "aic")
hc$arcs

mmhc <- h2pc(studentgrades[,-(1:5)])
mmhc$arcs

mmpc <- mmpc(studentgrades[,-(1:5)])
mmpc$arcs # lots of pairs but no substance
})

chow <- chow.liu(studentgrades[,-(1:5)])
chow$arcs[chow$arcs[,1]=="STAT.230"]

mmpc$arcs[mmpc$arcs[,1]=="STAT.230"]
```

### Support Vector Machines
Many research papers online have said they get one of the best prediction from SVMs so I figured I had to try it out. Using the base package e1071 support vector machines were implemented, however the predictions were not as good as we imagined.
```{r SVM model, eval=FALSE, echo=FALSE}
library(e1071)
library(missForest)

course_names <- c("COSC.121","MATH.200","MATH.221","COSC.221","COSC.211","MATH.101","COSC.222","ECON.101","COSC.111","PSYO.111","STAT.230")

grades <- studentgrades[!is.na(studentgrades$STAT.230),colnames(studentgrades) %in% course_names]

#grades.imputed <- missForest(grades)$ximp


n_rows <- nrow(grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

xtrain <- grades[train_indices,!names(grades) %in% c("STAT.230")]
ytrain <- grades[train_indices,"STAT.230"]
xtest <- grades[test_indices,!names(grades) %in% c("STAT.230")]
ytest <- grades[test_indices,"STAT.230"]


svm_model <- svm(STAT.230~., data=grades[train_indices,])

#opt_svm <- tune(svm_model, STAT.230~., data=grades[train_indices,],ranges=list(elsilon=seq(0,1,0.1), cost=1:100))

pred_svm <- predict(svm_model, data=grades[test_indices[1],])
length(pred_svm)
pred_svm

grades.real <- grades[test_indices,"STAT.230"]

rmse <- sqrt(mean((pred_svm - grades.real)^2))
rmse
```

We figured that with traincontrol and some hyperparameter tuning we would get some better results. Fortunately we did get good results, including an rmse on par with GBMs. This was not initially tried with the professor data and may be worth revisiting.
```{r SVM via trainControl, eval=TRUE, echo=FALSE}
course_names <- c("COSC.121","MATH.200","MATH.221","COSC.221","COSC.211","MATH.101","COSC.222","ECON.101","COSC.111","PSYO.111","STAT.230")

grades <- studentgrades[!is.na(studentgrades$STAT.230),colnames(studentgrades) %in% course_names]
#grades <- studentgrades[!is.na(studentgrades$STAT.230),]

grades <- missForest(grades)$ximp


n_rows <- nrow(grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

xtrain <- grades[train_indices,!names(grades) %in% c("STAT.230")]
ytrain <- grades[train_indices,"STAT.230"]
xtest <- grades[test_indices,!names(grades) %in% c("STAT.230")]
ytest <- grades[test_indices,"STAT.230"]


#To train the model we will use k-fold cross validation, with k set to 5
ctrl <- trainControl(method = "cv", number=5) 


# Create a grid of parameters to test and train the model with dimension 1
SVRGridCoarse <- expand.grid(.sigma=c(0.001, 0.01, 0.1), .C=c(10,100,1000))
SVRFitCoarse <- train(xtrain, ytrain, method="svmRadial", tuneGrid=SVRGridCoarse, trControl=ctrl, type="eps-svr")

# Display results
SVRFitCoarse

ggplot(SVRFitCoarse)


yPred <- predict(SVRFitCoarse$finalModel, xtest)

rmse <- sqrt(mean((yPred - ytest)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "MissForest impute into Support Vector Machines (on studentgrades)"))

predictions <- yPred
actual <- ytest
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

### Recursive Feature Elimination
This chunk was meant to be an attempt on using recursive feature elimination. However, this method doesn't work for data with NA values and couldn't be implemented with studentgrades.
```{r Trying RFE... doesnt work on NA, eval=FALSE, echo=FALSE}
library("faux")
library("DataExplorer")


studentgrades.factor <- studentgrades %>%
  # Save categorical features as factors
  mutate_at(c("Student_ID", "Major.1", "Major.2", "Minor", "Honors"), 
            as.factor)


# Define the control using a random forest selection function
control <- rfeControl(functions = rfFuncs, # random forest
                      method = "repeatedcv", # repeated cv
                      repeats = 5, # number of repeats
                      number = 10) # number of folds

n_rows <- nrow(studentgrades.factor)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

xtrain <- studentgrades.factor[train_indices,!names(studentgrades.factor) %in% c("STAT.230")]
ytrain <- studentgrades.factor[train_indices,"STAT.230"]
xtest <- studentgrades.factor[test_indices,!names(studentgrades.factor) %in% c("STAT.230")]
ytest <- studentgrades.factor[test_indices,"STAT.230"]

# Run RFE
result_rfe1 <- rfe(x = xtrain, 
                   y = ytrain, 
                   sizes = c(1:13),
                   rfeControl = control)

# Print the results
result_rfe1

# Print the selected features
predictors(result_rfe1)

# Print the results visually
ggplot(data = result_rfe1, metric = "Accuracy") + theme_bw()
ggplot(data = result_rfe1, metric = "Kappa") + theme_bw()
```

### LASSO Feature Selection
In this section there are two incomplete chunks of code. I had moved off of working on feature selection as we deemed GBMs were doing a good job at indicating which courses are the most important. 
lasso regression doesn't handle NA values so I tried to avoid that by using na.action = na.exclude. However, that didn't work. Error: Every row has at least one missing value were found
```{r LASSO feature selection, eval=FALSE, echo=FALSE}
cv_5 <- trainControl(method="cv", number=5)
studentgrades_stat230 <- studentgrades[!is.na(studentgrades$STAT.230),]
lasso <- train(STAT.230 ~., data=studentgrades_stat230, method='lasso',  trControl=cv_5, na.action = na.exclude)
```

```{r Lasso.., eval=FALSE, echo=FALSE}
library(glmnet)

cv_model <- cv.glmnet(studentgrades$STAT.230~studentgrades[,-c(1,2,3,4,5,"STAT.230")], nfolds = 10,alpha=1)
```

### Singular Value Decomposition
I had moved away from trying different modeling methods to focus on improving GBMS.
```{r SVD, eval=FALSE, echo=FALSE}
library(svd)

trlan.eigen(studentgrades[,-(1:5)])
```




## Further work on GBM
### Summary of the Boosting Process

1. **Initialize the model** with a constant value, typically the mean of the target values for regression:

\[ 
F_0(x) = \arg\min_{c} \sum_{i=1}^{N} L(y_i, c)
\]

2. **For \(m = 1\) to \(M\) (number of boosting iterations)**:
   - Compute the residuals \(r_i^m\)
   - For squared error loss, this simplifies to:
\[
r_i^m = y_i - F_{m-1}(x_i)
\]
   - Fit a base learner \(h_m(x)\) to the residuals.
   - Update the model via gradient decent:

\[
F_{m}(x) = F_{m-1}(x) + \nu h_m(x)
\]

3. **Final prediction** is the sum of all base learners:

\[
\hat{y} = F_M(x) = \sum_{m=1}^{M} \nu h_m(x)
\]


- What we are controlling and changing:
    - Boosting iterations (\texttt{ntrees}): variable...
    - Learning rate (\texttt{shrinkage} parameter): 0.05
    - Loss function distribution: Gaussian


### Data Preperation
To prepare the data fror gradient boosting, I did some cleaning to ensure the data used for training and testing would be available at the time of prediction for the student. For example, a student who wishes to predict their future STAT 230 grade wouldn't typically have 3rd or 4th year courses already completed. And definately wouldn't have grades for any courses with STAT 230 as a prereq.
In this chunk we remove data which doesn't add to the training process. Immediately this is student ID, a students' 2nd major, and whether they're in honors (this could be included but isn't for now). After removing all students who don't have the pre requeset courses, columns (courses) are dropped if they are make up of too many NA values. In our case we want at least 20% of the data to be non-NA. This number was made arbitrarily to reduce the number of courses trained on. The best level of NA values used would be different depending on the course being predicted on. After that, we removed all courses from higher year levels as we wouldn't expect them to be taken.
```{r GBM data prep COSC.221, eval=FALSE, echo=FALSE}
# Identifying the course of interest
COI <- "COSC.221"
coursegrades <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 50%
coursegrades <- coursegrades[coursegrades$COSC.221 >= 40, ]
# coursegrades <- coursegrades[coursegrades$COSC.221 <= 80, ]

# factoring columns
coursegrades$Major.1 <- factor(coursegrades$Major.1)
coursegrades$Minor <- factor(coursegrades$Minor)
coursegrades$Gender <- factor(coursegrades$Gender)
coursegrades$Citizenship <- factor(coursegrades$Citizenship)
coursegrades$Residency <- factor(coursegrades$Residency)

# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$MATH.101) | !is.na(coursegrades$MATH.103) | !is.na(coursegrades$MATH.142), ]





# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]
student_columns <- colnames(coursegrades)[1:5]




# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# display data to be used
head(coursegrades)
```

This chunk is for initializing STAT 230 GBM code as this was the main course I was testing on. It prepares a data frame to be passed into the GBM model for training and testing. The first data frame used includes students' grades (without the new rows added) and professor data.
```{r GBM data prep STAT230, eval=TRUE, echo=FALSE}
# Identifying the course of interest
COI <- "STAT.230"
coursegrades <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades <- coursegrades[coursegrades$STAT.230 >= 5, ]

# factoring columns
coursegrades$Major.1 <- factor(coursegrades$Major.1)
coursegrades$Minor <- factor(coursegrades$Minor)

# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]
student_columns <- colnames(coursegrades)[c(1:2)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)[6:length(colnames(coursegrades))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# display data to be used
kable(coursegrades[1:5, 1:10])
```




### Analysis
In this chunk we run the entire data set through a GBM which trains the data in its entirety (there is 10-fold cross validation). From there we can find the performance of the GBM using both cross validation and OOB to display the optimal number of iterations. We determine that with a learning rate of 0.001 there is no need for any more than 8000 iterations.
```{r finding ideal number of GBM tree graphs to use, eval=FALSE, echo=FALSE}

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 10000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(MATH.200 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)


print(gbm.perf(gbm_model, method="cv"))
print(gbm.perf(gbm_model, method="OOB"))
```

The optimal number of iterations is used when training the model to ensure efficiency. CV folds is the only thing which hasn't been strenuously tested to find the optimal number of folds. Additionally, we print out the model summary, use the model to predict grades, and calculate the rmse of the predicted grades.

```{r GBM model training, eval=TRUE, warning=FALSE, echo=FALSE}
set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing


gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 0.9,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# saving optimal # of trees
ntrees <- gbm.perf(gbm_model,method="cv")[1]



# Running model a second time with optimal # of trees
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

# save real grades
actual <- coursegrades[test_indices,]$STAT.230

rmse <- sqrt(mean((predictions - actual)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Gradient Boosted Machine (on studentgrades_prof)"))


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

#### GBM Comparisons
The first one is a gradient boosted model performed on the new studentgrades data with professors included.
```{r GBM on studentgrades_prof_new, eval=TRUE, echo=FALSE}
# Identifying the course of interest
COI <- "STAT.230"
coursegrades <- studentgrades_prof_new[!is.na(studentgrades_prof_new[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades <- coursegrades[coursegrades$STAT.230 >= 5, ]

# factoring columns
coursegrades$Major.1 <- factor(coursegrades$Major.1)
coursegrades$Minor <- factor(coursegrades$Minor)
coursegrades$Gender <- factor(coursegrades$Gender)
coursegrades$Citizenship <- factor(coursegrades$Citizenship)
coursegrades$Residency <- factor(coursegrades$Residency)


# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]

# coursegrades <- coursegrades[!is.na(coursegrades$MATH.101), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]
student_columns <- colnames(coursegrades)[c(1:5)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)[6:length(colnames(coursegrades))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# display data to be used
kable(coursegrades[1:5,1:10])

set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing


gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 0.9,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# saving optimal # of trees
ntrees <- gbm.perf(gbm_model,method="cv")[1]



# Running model a second time with optimal # of trees
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

# save real grades
actual <- coursegrades[test_indices,]$STAT.230

rmse <- sqrt(mean((predictions - actual)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Gradient Boosted Machine (on studentgrades_prof_new)"))


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

This GBM model uses both original and new students' grades without the professor data.
```{r GBM on studentgrades_new, eval=TRUE, echo=FALSE}
# Identifying the course of interest
COI <- "STAT.230"
coursegrades <- studentgrades_new[!is.na(studentgrades_new[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades <- coursegrades[coursegrades$STAT.230 >= 5, ]

# factoring columns
coursegrades$Major.1 <- factor(coursegrades$Major.1)
coursegrades$Minor <- factor(coursegrades$Minor)
coursegrades$Gender <- factor(coursegrades$Gender)
coursegrades$Citizenship <- factor(coursegrades$Citizenship)
coursegrades$Residency <- factor(coursegrades$Residency)


# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]

# coursegrades <- coursegrades[!is.na(coursegrades$MATH.101), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]
student_columns <- colnames(coursegrades)[c(1:5)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)[6:length(colnames(coursegrades))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# display data to be used
kable(coursegrades[1:5,1:10])

set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing


gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 0.9,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# saving optimal # of trees
ntrees <- gbm.perf(gbm_model,method="cv")[1]



# Running model a second time with optimal # of trees
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

# save real grades
actual <- coursegrades[test_indices,]$STAT.230

rmse <- sqrt(mean((predictions - actual)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Gradient Boosted Machine (on studentgrades_new)"))


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

The final GBM model uses the original studentgrades data set with no professors.
```{r GBM on studentgrades, eval=TRUE, echo=FALSE}
# Identifying the course of interest
COI <- "STAT.230"
coursegrades <- studentgrades[!is.na(studentgrades[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades <- coursegrades[coursegrades$STAT.230 >= 5, ]

# factoring columns
coursegrades$Major.1 <- factor(coursegrades$Major.1)
coursegrades$Minor <- factor(coursegrades$Minor)


# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]

# coursegrades <- coursegrades[!is.na(coursegrades$MATH.101), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]
student_columns <- colnames(coursegrades)[c(1:2)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)[6:length(colnames(coursegrades))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# display data to be used
kable(coursegrades[1:5,1:10])

set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing


gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 0.9,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# saving optimal # of trees
ntrees <- gbm.perf(gbm_model,method="cv")[1]



# Running model a second time with optimal # of trees
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

# save real grades
actual <- coursegrades[test_indices,]$STAT.230

rmse <- sqrt(mean((predictions - actual)^2))
all_model_rmse <- rbind(all_model_rmse, c(rmse, "Gradient Boosted Machine (on studentgrades)"))


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```


### Hyperparameter Training

Here we setup the GBM data for a few courses to find the best hyperparameters across many different courses.
```{r GBM data prep many courses, eval=FALSE, echo=FALSE}
# STAT 230
COI <- "STAT.230"
coursegrades <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades <- coursegrades[coursegrades$STAT.230 >= 5, ]

# factoring columns
coursegrades$Major.1 <- factor(coursegrades$Major.1)
coursegrades$Minor <- factor(coursegrades$Minor)

# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]
student_columns <- colnames(coursegrades)[c(1:2)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)[6:length(colnames(coursegrades))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# display data to be used
kable(coursegrades[1:5,1:10])


# COSC 221
COI <- "COSC.221"
coursegrades.c221 <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades.c221 <- coursegrades.c221[coursegrades.c221$COSC.221 >= 5, ]

# factoring columns
coursegrades.c221$Major.1 <- factor(coursegrades.c221$Major.1)
coursegrades.c221$Minor <- factor(coursegrades.c221$Minor)

# removing all students who don't meet course pre reqs DATA 101, COSC 221
# coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades.c221)) {
  if(is.na(coursegrades.c221[,i]) |> sum()  > nrow(coursegrades.c221)*pct){
    courses <- c(courses,i)
  }
}

coursegrades.c221 <- coursegrades.c221[,-courses]
student_columns <- colnames(coursegrades.c221)[c(1:2)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades.c221)[6:length(colnames(coursegrades.c221))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades.c221 <- coursegrades.c221[, colnames(coursegrades.c221) %in% cols_to_keep]

# display data to be used
coursegrades.c221


# DATA 311
COI <- "DATA.311"
coursegrades.d311<- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades.d311 <- coursegrades.d311[coursegrades.d311$DATA.311 >= 5, ]

# factoring columns
coursegrades.d311$Major.1 <- factor(coursegrades.d311$Major.1)
coursegrades.d311$Minor <- factor(coursegrades.d311$Minor)

# removing all students who don't meet course pre reqs DATA 101, COSC 221
# coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades.d311)) {
  if(is.na(coursegrades.d311[,i]) |> sum()  > nrow(coursegrades.d311)*pct){
    courses <- c(courses,i)
  }
}

coursegrades.d311 <- coursegrades.d311[,-courses]
student_columns <- colnames(coursegrades.d311)[c(1:2)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades.d311)[6:length(colnames(coursegrades.d311))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades.d311 <- coursegrades.d311[, colnames(coursegrades.d311) %in% cols_to_keep]

# display data to be used
coursegrades.d311


# DATA 301
COI <- "DATA.301"
coursegrades.d301 <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades.d301 <- coursegrades.d301[coursegrades.d301$DATA.301 >= 5, ]

# factoring columns
coursegrades.d301$Major.1 <- factor(coursegrades.d301$Major.1)
coursegrades.d301$Minor <- factor(coursegrades.d301$Minor)

# removing all students who don't meet course pre reqs DATA 101, COSC 221
# coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades.d301)) {
  if(is.na(coursegrades.d301[,i]) |> sum()  > nrow(coursegrades.d301)*pct){
    courses <- c(courses,i)
  }
}

coursegrades.d301 <- coursegrades.d301[,-courses]
student_columns <- colnames(coursegrades.d301)[c(1:2)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades.d301)[6:length(colnames(coursegrades.d301))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades.d301 <- coursegrades.d301[, colnames(coursegrades.d301) %in% cols_to_keep]

# display data to be used
coursegrades.d301


# MATH 221
COI <- "MATH.221"
coursegrades.m221 <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades.m221 <- coursegrades.m221[coursegrades.m221$MATH.221 >= 5, ]

# factoring columns
coursegrades.m221$Major.1 <- factor(coursegrades.m221$Major.1)
coursegrades.m221$Minor <- factor(coursegrades.m221$Minor)

# removing all students who don't meet course pre reqs DATA 101, COSC 221
# coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades.m221)) {
  if(is.na(coursegrades.m221[,i]) |> sum()  > nrow(coursegrades.m221)*pct){
    courses <- c(courses,i)
  }
}

coursegrades.m221 <- coursegrades.m221[,-courses]
student_columns <- colnames(coursegrades.m221)[c(1:2)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades.m221)[6:length(colnames(coursegrades.m221))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades.m221 <- coursegrades.m221[, colnames(coursegrades.m221) %in% cols_to_keep]

# display data to be used
coursegrades.m221



# STAT 303
COI <- "STAT.303"
coursegrades.s303 <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades.s303 <- coursegrades.s303[coursegrades.s303$STAT.303 >= 5, ]

# factoring columns
coursegrades.s303$Major.1 <- factor(coursegrades.s303$Major.1)
coursegrades.s303$Minor <- factor(coursegrades.s303$Minor)

# removing all students who don't meet course pre reqs DATA 101, COSC 221
# coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades.s303)) {
  if(is.na(coursegrades.s303[,i]) |> sum()  > nrow(coursegrades.s303)*pct){
    courses <- c(courses,i)
  }
}

coursegrades.s303 <- coursegrades.s303[,-courses]
student_columns <- colnames(coursegrades.s303)[c(1:2)]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades.s303)[6:length(colnames(coursegrades.s303))]

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- c(student_columns, column_names[column_digits <= YOI])

# All columns in the same year or below
coursegrades.s303 <- coursegrades.s303[, colnames(coursegrades.s303) %in% cols_to_keep]

# display data to be used
coursegrades.s303
```

Here we used trainControl to perform a grid search on the hyperparameters of the GBM algorithm. The main parameters are: n.trees - The number of iterations (decision trees built). interaction.depth - The maximum depth of decision trees. shrinkage - Learning rate (lower rate means it learns less per iteration), if this is too small the model is prone to overfitting. n.minobsinnode - minimum number of observations in terminal nodes. From this we found out a shrinkage of 0.001 or 0.01 is best, a interaction depth of 3-4 is best, the number of trees doesn't matter as we find the optimal # anyways, and n.minobsinnode doesn't have a large impact. Also, train.fraction and bag.fraction of 0.5 are best to ensure some randomness in the model.
```{r gridSearch on GBM, eval=FALSE, echo=FALSE}

train_control = trainControl(method = "cv", number = 5, search = "grid")

gbmGrid <-  expand.grid(
  # distribution = "gaussian",  # Specify the distribution for regression
  n.trees = c(6000),               # Number of trees (iterations)
  interaction.depth = c(2:6),       # Maximum depth of trees
  shrinkage = c(0.01,0.005,0.0025,0.001),           # Learning rate (shrinkage)
  # bag.fraction = 0.5,          # Fraction of training data used for each tree
  # train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = c(5,7,10,12)          # Minimum number of observations in terminal nodes
)

# training a Gboost Regression tree model while tuning parameters
# saving results into rdata files
# stat230.model = train(STAT.230~., data = coursegrades, method = "gbm", trControl = train_control, tuneGrid = gbmGrid, na.action = na.pass)
# save(stat230.model, file = "stat230model.RDATA")
# rm(stat230.model)
# cosc221.model = train(COSC.221~., data = coursegrades.c221, method = "gbm", trControl = train_control, tuneGrid = gbmGrid, na.action = na.pass)
# save(cosc221.model, file = "cosc221model.RDATA")
# rm(cosc221.model)
# data311.model = train(DATA.311~., data = coursegrades.d311, method = "gbm", trControl = train_control, tuneGrid = gbmGrid, na.action = na.pass)
# save(data311.model, file = "data311model.RDATA")
# rm(data311.model)
# data301.model = train(DATA.301~., data = coursegrades.d301, method = "gbm", trControl = train_control, tuneGrid = gbmGrid, na.action = na.pass)
# save(data301.model, file = "data301model.RDATA")
# rm(data301.model)


math221.model = train(MATH.221~., data = coursegrades.m221, method = "gbm", trControl = train_control, tuneGrid = gbmGrid, na.action = na.pass)
save(math221.model, file = "math221model.RDATA")
stat303.model = train(STAT.303~., data = coursegrades.s303, method = "gbm", trControl = train_control, tuneGrid = gbmGrid, na.action = na.pass)
save(stat303.model, file = "stat303model.RDATA")

load("stat230model.RDATA")
load("cosc221model.RDATA")
load("data311model.RDATA")
load("data301model.RDATA")
load("math221model.RDATA")
load("stat303model.RDATA")

# summarising the results
print(stat230.model$results)
print(cosc221.model$results)
print(data311.model$results)
print(data301.model$results)
print(math221.model$results)
print(stat303.model$results)


stat230.model$results[order(stat230.model$results[,"RMSE"]),]
cosc221.model$results[order(cosc221.model$results[,"RMSE"]),]
data311.model$results[order(data311.model$results[,"RMSE"]),]
data301.model$results[order(data301.model$results[,"RMSE"]),]
math221.model$results[order(math221.model$results[,"RMSE"]),]
stat303.model$results[order(stat303.model$results[,"RMSE"]),]

# Make predictions on new data
# predictions <- predict(model$bestTune, newdata = coursegrades[test_indices,], n.trees = ntrees)
# 
# rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
# rmse
```



### Adding Error Bars
This chunk adds error bars onto the graph of predicted vs actual grades. The error bars are calculated by taking the standard deviation of the absolute value of prediction grades - actual grades.

```{r adding error bars, eval=TRUE, echo=FALSE}
## calculating size of error bars
errors <- abs(predictions - actual)
error_bars <- sd(errors)



## plotting graph and error bars
plot(actual, predictions, 
     xlab = "Actual Grades", ylab = "Predicted Grades",
     main = "Predicted vs Actual Grades with Error Bars", xlim = c(0,100), ylim = c(0,100),pch = 16)

# Add error bars
arrows(actual, predictions - error_bars, 
       actual, predictions + error_bars, 
       angle = 90, code = 3, length = 0.1, col = 1:length(predictions))

abline(0, 1, col = "red")
```

This chunk follows another paper in adding quantile error bars. These are calculated using the "quantile" distribution as a parameter to the gbm. Both upper and lower bounds on the quantile range are found a plotted alongside predictions.

```{r quantile gbm error bars, eval=TRUE, echo=FALSE}
## training two more gbms using quantile ranges as the prediction interval

gbm_params <- list(
  distribution = list(name = "quantile", alpha = 0.025),  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

LB_predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)


gbm_params <- list(
  distribution = list(name = "quantile", alpha = 0.975),  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

UB_predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

## plotting graph and error bars
plot(actual, predictions, 
     xlab = "Actual Grades", ylab = "Predicted Grades",
     main = "Predicted vs Actual Grades with Error Bars", xlim = c(00,100), ylim = c(0,100),pch = 16)

# Add error bars
arrows(actual, LB_predictions, 
       actual, UB_predictions, 
       angle = 90, code = 3, length = 0.1, col = 1:length(predictions))

abline(0, 1, col = "red")
```

Our final way of calculating error bars is by bootstrapping the model 50 times and using the standard deviation to calculate the upper and lower bound for each grade predicted.

```{r bootstrap gbm error bars, eval=TRUE, echo=FALSE}
# bootstrapping the model x times
n_iterations = 50  # Number of bootstrapped models
boot_preds = numeric(0)


gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 6000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)


for(i in 1:n_iterations){
  # Create a bootstrapped dataset
  set.seed(100+i)
  
  n_rows <- nrow(coursegrades)
  train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
  test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing
    
  gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

  
  # Predict on test data
  preds <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)
  boot_preds <- append(boot_preds, preds)
  
}

# Convert predictions to a NumPy array
#predictions

# Calculate the mean and standard deviation of the predictions
mean_preds <- mean(boot_preds)
std_preds <- sd(boot_preds)

# Confidence interval on average grade of STAT 230
lower_bound <- mean_preds - 1.96 * std_preds
upper_bound <- mean_preds + 1.96 * std_preds


## plotting graph and error bars made from bootstrapping predictions from many different models
UB_predictions <- predictions + 1.96 * std_preds
LB_predictions <- predictions - 1.96 * std_preds


plot(actual, predictions, 
     xlab = "Actual Grades", ylab = "Predicted Grades",
     main = "Predicted vs Actual Grades with Error Bars", xlim = c(00,100), ylim = c(0,100),pch = 16)

# Add error bars
arrows(actual, LB_predictions, 
       actual, UB_predictions, 
       angle = 90, code = 3, length = 0.1, col = 1:length(predictions))

abline(0, 1, col = "red")
```


### Variable selection
This variable selection technique was taken from a **paper** with some modifications to see if removing variables improves the model. After training the model on all predictors, the two given the least importance by the gbm algorithm are removed and the model is trained again until either 2 or 1 predictors remain. For the first few courses removed it performed alright, with a rmse of ~9.5. However, as we can see the rmse never got any better and slowly increased to 10.8. 
```{r GBM recursive variable selection on prof data, eval=FALSE, echo=FALSE}

set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing


gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# saving optimal # of trees
ntrees <- gbm.perf(gbm_model,method="cv")[1]



# Running model a second time with optimal # of trees
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)


# Make predictions on new data
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
rmse

# making df to save columns removed and rmse
model_perf <- data.frame(RMSE = rmse, col1 = NA, col2 = NA)








# creating a temp coursegrades which we will modify
temp_coursegrades <- coursegrades

# main loop to filter variables
for(loop in 1:floor((ncol(coursegrades)-3)/2)){
  
  # getting relative influence of variables
  col_inf <- matrix(relative.influence(gbm_model))
  col_inf_names <- gbm_model$var.names
  
  rel_inf <- data.frame(influence = col_inf, column = col_inf_names)
  
  rel_inf <- rel_inf[order(rel_inf$influence, decreasing = FALSE),]
  
  
  # find columns to remove
  cols_to_remove <- rel_inf[c(1,2),2]
  for(i in 1:2){
    col <- grep(cols_to_remove[i], colnames(temp_coursegrades))[1]
    temp_coursegrades <- temp_coursegrades[,-col]
  }
  
  set.seed(1234)
  n_rows <- nrow(temp_coursegrades)
  train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
  test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing
  
  
  
  # Running model first time on base params
  gbm_model <- gbm(STAT.230 ~ ., data = temp_coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)

  # saving optimal # of trees
  ntrees <- gbm.perf(gbm_model,method="cv")[1]



  # Running model a second time with optimal # of trees
  gbm_model <- gbm(STAT.230 ~ ., data = temp_coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)
  
  
  print(summary(gbm_model))
  
  # Make predictions on new data
  predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

  rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
  
  model_perf <- rbind(model_perf, c(rmse, cols_to_remove))
  
}

save(model_perf, file = "model_perf.RData")
```

This table shows the models performance as the 'least relevant' courses are removed. Courses are rated to each other based on their relative influence on the model. We take the two courses listed as having the least relative influence and remove them from the next model run. This was repeated until no more courses remained. The table below shows the models test rmse. The two courses in each row were removed prior to training the model. The first row shows the performance with no columns removed. Note: this is additional variable selection performed after removing all columns with more than 80% NA values.
```{r GBM rvs, eval=TRUE, echo=FALSE}
load("model_perf.RData")
plot(model_perf$RMSE)
```

The output has been altered to show the RMSE of the model after two courses are consecutively removed from training/testing process.


## Extreme Gradient Boosting
Both Gradient Boosting Machines and Extreme Gradient Boosting follow the sample principle of gradient boosting. However, xgboost was designed to use a more regularized model formalization to control over-fitting. While this gives it better performance, I had more luck coding GBMS. For xgboost the hyperparameter training had to be set up very differently and with new parameters such as lambda and alpha training took considerably longer. Additionally, GBMs were found to be easier to set up each time allowing for the course we predict on to be changed easily.
```{r extreme gradient boosting, eval=FALSE, echo=FALSE}
library(xgboost)

set.seed(5934)
# set.seed(555)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

## xgbm data setup
train_data <- xgb.DMatrix(as.matrix(coursegrades[train_indices,!names(coursegrades) %in% c("STAT.230")]), label = coursegrades[train_indices,"STAT.230"])
test_data <- xgb.DMatrix(as.matrix(coursegrades[test_indices,!names(coursegrades) %in% c("STAT.230")]), label = coursegrades[test_indices,"STAT.230"])



watchlist = list(train=train_data, test=test_data)

xgbm_model <- xgb.train(data=train_data, max.depth=4, eta=0.01, nthread = 2, nrounds=1000, lambda = 0, booster = "gbtree", subsample = 0.5, tree_method = "auto", watchlist=watchlist, objective = "reg:squarederror", verbose = 0, early_stopping_rounds = 10, colsample_bytree = 0.5)

# Print the summary of the trained model
summary(xgbm_model)


# Make predictions on new data
predictions <- predict(xgbm_model, test_data)

rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
rmse
```

This is the hyperparameter gridsearch for the xgboost algorithm. This is the chunk that showed that a subsample and colsample_bytree of 0.5 were best (bag.fraction and train.fraction from gbm hyperparameters).
```{r gridSearch on xGBM, eval=FALSE, echo=FALSE}
library(caret)
library(tidyverse)

searchGridSubCol <- expand.grid(subsample = c(0.5, 0.65, 0.8), 
                                colsample_bytree = c(0.5, 0.75, 1),
                                max_depth = c(1, 2),
                                lambda = c(0,1,2), 
                                alpha = c(0,1,2),
                                min_child = c(1,3),
                                eta = c(0.05, 0.01, 0.005, 0.001)
)

ntrees <- 10000

system.time(
rmseErrorsHyperparameters <- apply(searchGridSubCol, 1, function(parameterList){
  
  #Extract Parameters to test
  currentSubsampleRate <- parameterList[["subsample"]]
  currentColsampleRate <- parameterList[["colsample_bytree"]]
  currentDepth <- parameterList[["max_depth"]]
  currentEta <- parameterList[["eta"]]
  currentLambda <- parameterList[["lambda"]]
  currentAlpha <- parameterList[["alpha"]]
  currentMinChild <- parameterList[["min_child"]]
  xgboostModelCV <- xgb.cv(data =  train_data, nrounds = ntrees, nfold = 5, showsd = TRUE, 
                       metrics = "rmse", verbose = TRUE, "eval_metric" = "rmse",
                     "objective" = "reg:linear", "max.depth" = currentDepth, "eta" = currentEta,                               
                     "subsample" = currentSubsampleRate, "colsample_bytree" = currentColsampleRate
                      , print_every_n = 10, "lambda" = currentLambda, "alpha" = currentAlpha, "min_child" = currentMinChild, booster = "gbtree",
                     early_stopping_rounds = 10)
  
  xvalidationScores <- as.data.frame(xgboostModelCV$evaluation_log)
  rmse <- tail(xvalidationScores$test_rmse_mean, 1)
  trmse <- tail(xvalidationScores$train_rmse_mean,1)
  output <- return(c(rmse, trmse, currentSubsampleRate, currentColsampleRate, currentDepth, currentEta, currentMinChild))}))


# Showing hyperparameter results
xgb_hyperparams <- data.frame(t(rmseErrorsHyperparameters))
xgb_hyperparams$lambda <- searchGridSubCol$lambda
xgb_hyperparams$alpha <- searchGridSubCol$alpha

colnames(xgb_hyperparams) <- c("rmse", "trmse", "subsample", "colsample_bytree", "max_depth", "eta", "min_child", "lambda", "alpha")

xgb_hyperparams[order(xgb_hyperparams$rmse),]
```

The only error bars which could be implemented are the bootstrapped model since xgboost doesn't support establishing the distribution of the boosting process like gbm.
```{r adding bootstrapped xGBM error bars, eval=FALSE, echo=FALSE}
# bootstrapping the model x times 

n_iterations = 100  # Number of bootstrapped models
predictions = numeric(0)

for(i in 1:n_iterations){
  # Create a bootstrapped dataset
  set.seed(100+i)
  
  n_rows <- nrow(coursegrades)
  train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
  test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing
    
  # Train an XGBoost model
  train_data <- xgb.DMatrix(as.matrix(coursegrades[train_indices,!names(coursegrades) %in% c("STAT.230")]), label = coursegrades[train_indices,"STAT.230"])
  test_data <- xgb.DMatrix(as.matrix(coursegrades[test_indices,!names(coursegrades) %in% c("STAT.230")]), label = coursegrades[test_indices,"STAT.230"])

  watchlist = list(train=train_data, test=test_data)

  xgbm_model <- xgb.train(data=train_data, max.depth=4, eta=0.01, nthread = 2, nrounds=1000, lambda = 0, booster = "gbtree", subsample = 0.5, tree_method = "auto", watchlist=watchlist, objective = "reg:squarederror", verbose = 0, early_stopping_rounds = 10, colsample_bytree = 0.5)
    
  # Predict on test data
  preds <- predict(xgbm_model, test_data)
  predictions <- append(predictions, preds)
  
  sprintf("%s done", i)
}

# Convert predictions to a NumPy array
#predictions

# Calculate the mean and standard deviation of the predictions
mean_preds = mean(predictions)
std_preds = sd(predictions)


## plotting graph and error bars made from bootstrapping predictions from many different models
UB_predictions <- predictions + 1.96 * std_preds
LB_predictions <- predictions - 1.96 * std_preds


plot(actual, predictions, 
     xlab = "Actual Grades", ylab = "Predicted Grades",
     main = "Predicted vs Actual Grades with Error Bars", xlim = c(00,100), ylim = c(0,100),pch = 16)

# Add error bars
arrows(actual, LB_predictions, 
       actual, UB_predictions, 
       angle = 90, code = 3, length = 0.1, col = 1:length(predictions))

abline(0, 1, col = "red")
```



## Main Plots Used
### Predicted grades vs Actual grades
The main plot we used to show progress was plotting the predicted student grades against their actual grades. This graph immediately shows us if we're over or under predicting due to the red line. This line indicates "perfect prediction" as it follows where predicted grades = actual grades.
```{r predicted grades vs actual grades, eval=TRUE, echo=FALSE}
actual <- coursegrades[test_indices,]$STAT.230
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

### Residuals
This was added in for model checking to make sure there are no trend in residuals.
```{r residuals, eval=TRUE, echo=FALSE}
residuals <- actual - predictions
plot(predictions, residuals, xlab = "Predicted Grades", ylab = "Residuals", 
     main = "Residual Plot", pch = 16, col = "blue")
abline(h = 0, col = "red")

line.lm <- lm(residuals~predictions)
abline(line.lm, col = "green")
```

### Q-Q Plots
A q-q plot was also added in for model checking.
```{r qq plots, eval=TRUE, echo=FALSE}
qqnorm(residuals, main = "Normal Q-Q Plot of Residuals")
qqline(residuals, col = "red")
```

### Heatmap
The heatmaps were added to view the data more categorically.
This first chunk splits predictions and actual grades into intervals of 5 (eg. 50-54, 55-59...) and produces a heatmap of predicted vs actual grades.
```{r conf matrix and heat map, eval=TRUE, echo=FALSE}

# Define breaks for intervals of 5
breaks <- seq(0, 100, by = 5)

# Create labels for the intervals
labels <- paste(breaks[-length(breaks)], breaks[-1], sep = "-")

actual_fac <- cut(actual, breaks = breaks, labels = labels, right = TRUE, include.lowest = TRUE)
pred_fac <- cut(predictions, breaks = breaks, labels = labels, right = TRUE, include.lowest = TRUE)

confusion_matrix <- table(actual_fac, pred_fac)


confusion_matrix <- confusionMatrix(confusion_matrix)
# Print the confusion matrix
# confusion_matrix

# Print recall for each class
rec <- confusion_matrix$byClass[, "Recall"]
prec <- confusion_matrix$byClass[, "Precision"]
f1_score <- 2*prec*rec/(prec+rec)

confusion_matrix$Freq[is.na(confusion_matrix$Freq)] <- 0

# Create the heat map
ggplot(as.data.frame(confusion_matrix$table), aes(x = actual_fac, y = pred_fac, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "red") +
  labs(x = "Actual Grades", y = "Predicted Grades", fill = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_tile(data = subset(as.data.frame(confusion_matrix$table), actual_fac == pred_fac),
            aes(fill = Freq), color = "black")
```

The second chunk splits predictions and actual grades into their letter grades and produces a heatmap of predicted vs actual grades using those splits.
```{r Heatmap on prediction letter grades, eval=TRUE, echo=FALSE}

# Function to assign letter grades based on numeric ranges
assign_grades <- function(scores) {
  # Define the breaks for the numeric ranges
  breaks <- c(-Inf, 49, 54, 59, 63, 67, 71, 75, 79, 84, 89, 100)
  
  # Define the corresponding labels for each range
  labels <- c("F", "D", "C-", "C", "C+", "B-", "B", "B+", "A-", "A", "A+")
  
  # Use the cut function to assign labels based on the breaks
  grades <- cut(scores, breaks = breaks, labels = labels, right = TRUE)
  
  return(grades)
}


predictions_letter <- assign_grades(predictions)
actual_letter <- assign_grades(actual)

# displaying confusion matrix
confusionMatrix(predictions_letter, actual_letter)


confusion_matrix <- table(actual_letter, predictions_letter)
confusion_matrix <- confusionMatrix(confusion_matrix)

# Creating and displaying heat map
ggplot(as.data.frame(confusion_matrix$table), aes(x = actual_letter, y = predictions_letter, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "red") +
  labs(x = "Actual Grades", y = "Predicted Grades", fill = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_tile(data = subset(as.data.frame(confusion_matrix$table), actual_letter == predictions_letter),
            aes(fill = Freq), color = "black")
```

# Model Comparison
Here we plot the rmse of each model performed with a brief description. All models were trained to predict a student's STAT 230 grade. The same train and test sets were used, with the exception of models using new student data.
```{r final model comparison, eval=TRUE, echo=FALSE}
colnames(all_model_rmse) <- c("RMSE", "Model Details")


plot(all_model_rmse$RMSE, ylim=c(6.5,15), ylab = "RMSE", xlab = "Model Index", main = "RMSE of Model")

all_model_rmse$`Model Details`[1] <- "[Linear Regression on Prerequisites](#initial-eda)"
all_model_rmse$`Model Details`[2] <- "[MissForest imputation as prediction](#missforest) ([studentgrades](#loading-studentgrades))"
all_model_rmse$`Model Details`[3] <- "[Random Forest on non-NA data](#random-forest) ([studentgrades_rep](#loading-studentgrades_rep))"
all_model_rmse$`Model Details`[4] <- "[Random Forest on non-NA data (studentgrades)](#random-forest)"
all_model_rmse$`Model Details`[5] <- "[Gradient Boosted Machines using 4 coureses as predictors (studentgrades_rep)](#gradient-boosted-machines)"
all_model_rmse$`Model Details`[6] <- "[Gradient Boosted Machines on complete cases of 4 courses (studentgrades_rep)](#gradient-boosted-machines)"
all_model_rmse$`Model Details`[7] <- "[MissForest imputation into GBM on 4 courses (studentgrades_rep)](#gradient-boosted-machines)"
all_model_rmse$`Model Details`[8] <- "[MissForest imputation into GBM on 4 courses (studentgrades)](#gradient-boosted-machines)"
all_model_rmse$`Model Details`[9] <- "[MissForest imputation into GBM on all courses (studentgrades)](#gradient-boosted-machines)"
all_model_rmse$`Model Details`[10] <- "[Matrix Factorization on the 199 courses taken the most times (studentgrades)](#GBM Comparisons)"
all_model_rmse$`Model Details`[11] <- "[MissForest impute into Support Vector Machines (on studentgrades)](#GBM Comparisons)"
all_model_rmse$`Model Details`[12]<- "[Gradient Boosted Machine (on studentgrades_prof)](#GBM Comparisons)"
all_model_rmse$`Model Details`[13] <- "[Gradient Boosted Machine (on studentgrades_prof_new)](#GBM Comparisons)"
all_model_rmse$`Model Details`[14] <- "[Gradient Boosted Machine (on studentgrades_new)](#GBM Comparisons)"
all_model_rmse$`Model Details`[15] <- "[Gradient Boosted Machine (on studentgrades)](#GBM Comparisons)"

all_model_rmse$`Model Details`[1] <- "[Linear Regression on Prerequisites](#initial-eda)"
all_model_rmse$`Model Details`[2] <- "[MissForest imputation as prediction](#missforest) ([studentgrades](#loading-studentgrades))"
all_model_rmse$`Model Details`[3] <- "[Random Forest on non-NA data](#random-forest) ([studentgrades_rep](#loading-studentgrades_rep))"
all_model_rmse$`Model Details`[4] <- "[Random Forest on non-NA data](#random-forest) ([studentgrades](#loading-studentgrades))"
all_model_rmse$`Model Details`[5] <- "[Gradient Boosted Machines using 4 courses as predictors](#gradient-boosted-machines) ([studentgrades_rep](#loading-studentgrades_rep))"
all_model_rmse$`Model Details`[6] <- "[Gradient Boosted Machines on complete cases of 4 courses](#gradient-boosted-machines) ([studentgrades_rep](#loading-studentgrades_rep))"
all_model_rmse$`Model Details`[7] <- "[MissForest imputation into GBM on 4 courses](#gradient-boosted-machines) ([studentgrades_rep](#loading-studentgrades_rep))"
all_model_rmse$`Model Details`[8] <- "[MissForest imputation into GBM on 4 courses](#gradient-boosted-machines) ([studentgrades](#loading-studentgrades))"
all_model_rmse$`Model Details`[9] <- "[MissForest imputation into GBM on all courses](#gradient-boosted-machines) ([studentgrades](#loading-studentgrades))"
all_model_rmse$`Model Details`[10] <- "[Matrix Factorization on the 199 courses taken the most times](#GBM Comparisons) ([studentgrades](#loading-studentgrades))"
all_model_rmse$`Model Details`[11] <- "[MissForest impute into Support Vector Machines](#GBM Comparisons) ([studentgrades](#loading-studentgrades))"
all_model_rmse$`Model Details`[12] <- "[Gradient Boosted Machine](#GBM Comparisons) ([studentgrades_prof](#loading-studentgrades_prof))"
all_model_rmse$`Model Details`[13] <- "[Gradient Boosted Machine](#GBM Comparisons) ([studentgrades_prof_new](#loading-studentgrades_prof_new))"
all_model_rmse$`Model Details`[14] <- "[Gradient Boosted Machine](#GBM Comparisons) ([studentgrades_new](#loading-studentgrades_new))"
all_model_rmse$`Model Details`[15] <- "[Gradient Boosted Machine](#GBM Comparisons) ([studentgrades](#loading-studentgrades))"


kable(all_model_rmse)
```



