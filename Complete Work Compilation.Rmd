---
title: "Complete Work"
author: "Ross Cooper 54907605"
date: "2024-07-10"
output: html_document
---


# Data Cleaning and Processing

## Student grades data cleaning

The first thing we do is read in the student grades from the csv given. After that we trim the whitespace for all character columns to ensure there are no irregularities. A course code column is created to put a course with it's course number in the form COURSE.NUMBER.
```{r reading in student-data, eval = FALSE}
## this chunk is used to read in student grades data from student-data.csv
## and cleans the data to be used
df <- read.csv("..\\UBCO-Grade-Prediction-data\\student-data.csv")

df_clean <- df[,c(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18)]

# Do some cleaning on chr columns
df_clean$STUD_NO_ANONYMOUS <- trimws(df_clean$STUD_NO_ANONYMOUS)
df_clean$CRS_DPT_CD <- trimws(df_clean$CRS_DPT_CD)
df_clean$HDR_CRS_LTTR_GRD <- trimws(df_clean$HDR_CRS_LTTR_GRD)
df_clean$CURR_SPEC_PRIM_PGM_TYPE_1 <- trimws(df_clean$CURR_SPEC_PRIM_PGM_TYPE_1)
df_clean$CURR_SPEC_PRIM_SUBJECT_1 <- trimws(df_clean$CURR_SPEC_PRIM_SUBJECT_1)
df_clean$CURR_SPEC_PRIM_PGM_TYPE_2 <- trimws(df_clean$CURR_SPEC_PRIM_PGM_TYPE_2)
df_clean$CURR_SPEC_PRIM_SUBJECT_2 <- trimws(df_clean$CURR_SPEC_PRIM_SUBJECT_2)
df_clean$CURR_SPEC_SECN_PGM_TYPE_1 <- trimws(df_clean$CURR_SPEC_SECN_PGM_TYPE_1)
df_clean$CURR_SPEC_SECN_SUBJECT_1 <- trimws(df_clean$CURR_SPEC_SECN_SUBJECT_1)
df_clean$CURR_SPEC_SECN_PGM_TYPE_2 <- trimws(df_clean$CURR_SPEC_SECN_PGM_TYPE_2)
df_clean$CURR_SPEC_SECN_SUBJECT_2 <- trimws(df_clean$CURR_SPEC_SECN_SUBJECT_2)
df_clean$DEGR_PGM_CD <- trimws(df_clean$DEGR_PGM_CD)
df_clean$SEC_SES_YR <- trimws(df_clean$SEC_SES_YR)
df_clean$SEC_NO <- trimws(df_clean$SEC_NO)
df_clean$SEC_SES_CD <- trimws(df_clean$SEC_SES_CD)


# Factor grades column
grades <-
  c("A+", "A", "A-", "B+", "B", "B-", "C+", "C", "C-", "D", "F")
df_clean$HDR_CRS_LTTR_GRD <-
  factor(df_clean$HDR_CRS_LTTR_GRD, levels = grades)

# Create course code column
df_clean$COURSE_CODE <- paste(df_clean$CRS_DPT_CD, df_clean$CRS_NO, sep = ".")
df_clean <- df_clean[,-(12:13)]

# Removing all withdrawl grades
df_clean <- df_clean[df_clean$HDR_CRS_PCT_GRD < 999,]

# Changing all withdrawls (999.9) to -1 to ensure they're kept in the data
#df_clean$HDR_CRS_PCT_GRD[df_clean$HDR_CRS_PCT_GRD == 999.9] <- -1

# Create year code column
df_clean$YEAR_CODE <- paste0(df_clean$SEC_SES_YR, df_clean$SEC_SES_CD)
df_clean <- df_clean[,-(15:16)]

# Removing 2023 grades (not included) [also does nothing bc ^ cleans it out]
#df_clean <- subset(df_clean, SEC_SES_YR < 2022.5)


# for if we want only BSc students (better to keep all data in)
#df_bsc <- df_clean[df_clean$DEGR_PGM_CD=="BSC-O",]


df_bsc
```

The student grades data has not fundamentally changed. 
```{r loading cleaned student-data, eval=TRUE}
load("student-data.RData")
head(df_bsc)
```

This chunks' main purpose is to make a table summing the number of times each course is taken. This also identifies the "Relevant Courses" as df_bsc_relevant_courses.
```{r finding number of times courses were taken, eval=FALSE}
# Determining how many bsc students took each course
unique(df_clean$COURSE_CODE)|>length()
# making a table indicating the number of times each course was taken
courses_taken_bsc <- table(df_bsc$COURSE_CODE)

# previous lines of code
# sorted_courses_taken_bsc <- sort(courses_taken_bsc, decreasing = TRUE)[1:200]
sorted_courses_taken_bsc <- sort(courses_taken_bsc, decreasing = TRUE)
# sorted_courses_taken_bsc <- sorted_courses_taken_bsc[substr(names(sorted_courses_taken_bsc),1,4) %in% c("DATA","MATH",
                                                   # "COSC","STAT")]

# current line which removes any courses not taken at least 10 times
# sorted_courses_taken_bsc <- sort(courses_taken_bsc[courses_taken_bsc >= 10], decreasing = TRUE)

# barplot showing # of times a course is taken
barplot(sorted_courses_taken_bsc)
# saving the course names of those which remain
course_names <- dimnames(sorted_courses_taken_bsc)[[1]]

# Subsetting on only courses which are taken 10 or more times ("relevant courses")
df_bsc_relevant_courses <- df_bsc[df_bsc$COURSE_CODE %in% course_names, ]

# Getting a list of all student ids which took relevant courses
uniqueids <- unique(df_bsc_relevant_courses$STUD_NO_ANONYMOUS)
```

This chunk was used to determine the maximum number of times each course was taken. With the number of times a course was taken, we can make enough columns for each students' retaken course grade without issue.
```{r counting course retake occurences, eval=FALSE}
# finding student who've taken a course more than once
duplicate_rows <- df_bsc_relevant_courses[duplicated(df_bsc_relevant_courses[c("STUD_NO_ANONYMOUS", "COURSE_CODE")]) | duplicated(df_bsc_relevant_courses[c("STUD_NO_ANONYMOUS", "COURSE_CODE")], fromLast = TRUE), ]
# duplicate_rows[c(1,15,12,13)]

# Count occurrences of each course for each student
course_counts <- table(duplicate_rows$STUD_NO_ANONYMOUS, duplicate_rows$COURSE_CODE)

# Find the maximum number of times each course was taken by any student
max_repeats <- apply(course_counts, 2, max)

# Combine course names with corresponding maximum counts
max_course_counts_df <- data.frame(COURSE_CODE = names(max_repeats), max_repeats)
```

Here we create those columns header names to be able to create a empty data frame to fill.
```{r creating list of course column names .1 .2, eval=FALSE}
# Creating a new list of course column names
## eg. Math.101.1, Math.101.2...

## want a list of course names with the .1 .2 .3
# groups: Non-Repeats, Repeats
# Non-repeats
non_rep_courses <- setdiff(course_names,max_course_counts_df$COURSE_CODE)

# Repeats
# Modify course names for repeated courses
rep_courses <- c()
for (i in 1:nrow(max_course_counts_df)) {
  for (j in 1:max_course_counts_df[i,2]) {
    rep_courses <- append(rep_courses, paste0(max_course_counts_df[i,1], ".", j))
  }
}

course_names_dups <- append(non_rep_courses, rep_courses)
```


### Extracting student majors and minors
This chunk is used to find each students' major(s), minor, and whether they are taking honors. They are saved in a data frame with each student on a new row. This data frame is later fed into the studentgrades data frame including all student grades. This chunk will be run for any of the three types of students grade structure used.
```{r finding student majors and minors, eval=FALSE}
# Finding student majors and minors
### WARNING: no 2023 data yet which may contain updates to major/minors


# looking through/ getting last row of each student
library(dplyr)

# Assuming your dataframe is called 'df'

# Group by student_id and get the last row of each group
last_rows <- df_bsc_relevant_courses %>%
  group_by(STUD_NO_ANONYMOUS) %>%
  slice(n())


# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(last_rows), ncol = 4))


# Add course name columns to existing data frame
student_mm <- cbind(last_rows[,1], new_columns)
colnames(student_mm) <- c("StudentID", "Major.1", "Major.2", "Minor", "Honors")
student_mm$Honors <- FALSE

for(i in 1:nrow(last_rows)){
  # If MAJ is in column 1, major.1 is that subject
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="MAJ"){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
  }
  
  # if MAJ is in col 2, check if major.1 is occupied and place it in maj.1 or maj.2
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="MAJ" & is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="MAJ"){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  }
  
  # combined majors....
  
  # finding that minor
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="MIN"){
    student_mm[i,]$Minor <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="MIN"){
    student_mm[i,]$Minor <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  }
  
  # adding Honors as majors
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="HON" & is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
    student_mm[i,]$Honors <- TRUE
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="HON" & is.na(student_mm[i,]$Major.2)){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
    student_mm[i,]$Honors <- TRUE
  }
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="HON" & is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
    student_mm[i,]$Honors <- TRUE
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="HON" & is.na(student_mm[i,]$Major.2)){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
    student_mm[i,]$Honors <- TRUE
  }
  
  # adding combined majors
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="CMJ"){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_SECN_SUBJECT_1
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="CMJ" & !is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  }
  
}
head(student_mm)
```




# Data Structure
## Using all student grades from retaken courses

This variation of studentgrades has each student represented as rows, and courses as columns. Course grades are recorded for each attempt a student takes at each course. Each new attempt is saved in a new column with all 3rd time attemps of MATH.101 being saved in MATH.101.3. For example, MATH 101 has 3 columns: MATH.101.1, MATH.101.2, MATH.101.3. One for each new attempt at MATH 101.

Here we create an empty data frame, add in the unique student ids and their matching majors and minors. After we loop through df_bsc_relevant_courses to add in grades to their respective student and course column.
```{r Making studentgrades with .1 .2 etc, eval=FALSE}
# Making the Data Frame
 ## Columns to add: Student ID, Major, Honors, Minor, Extras, 200 most common courses

# Building df after getting the data

studentgrades <- data.frame(Student_ID = sort(uniqueids))




# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(studentgrades), ncol = length(course_names_dups)))
colnames(new_columns) <- course_names_dups

# Add course name columns to existing data frame
studentgrades <- cbind(studentgrades, student_mm[,-1], new_columns)


# Saving student grades into df
for (i in 1:nrow(df_bsc_relevant_courses)){
  rep <- 0
  # finding the index of course name .1
  coursecol <- grep(df_bsc_relevant_courses[i,]$COURSE_CODE, colnames(studentgrades))[1]
  
  # Moving over from .1 to .2 columns if a grade already exists
  while(!is.na(studentgrades[studentgrades$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+rep])){
    rep = rep+1
  }
  
  # Saving student grade into studentgrades df
  studentgrades[studentgrades$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+rep] <- df_bsc_relevant_courses[i,]$HDR_CRS_PCT_GRD
}

studentgrades[,c("Student_ID","MATH.101.1","MATH.101.2")]


```

```{r loading studentgrades with .1 .2, eval=TRUE}
load(file = "studentgrades_1.RData")
head(studentgrades_rep)
```



## Using only the most recent course grades

This variation of studentgrades has each student represented as rows, and courses as columns. Only the most recent attempt at each course is recorded. For example, MATH 101 has only one column: MATH.101.

To do that we only create one column per course and save the most recent grade into correct student row and course column. This will overwrite existing grades if a student has retaken a course.
```{r Making studentgrades using most recent grade, eval=FALSE}
studentgrades <- data.frame(Student_ID = sort(uniqueids))

# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(studentgrades), ncol = length(course_names)))
colnames(new_columns) <- course_names

# Add course name columns to existing data frame
studentgrades <- cbind(studentgrades, student_mm[,-1], new_columns)


# Saving student grades into df
for (i in 1:nrow(df_bsc_relevant_courses)){
  
  # finding the index of course name .1
  coursecol <- grep(df_bsc_relevant_courses[i,]$COURSE_CODE, colnames(studentgrades))[1]
  
  # Saving student grade into studentgrades df
  studentgrades[studentgrades$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol] <- df_bsc_relevant_courses[i,]$HDR_CRS_PCT_GRD
}

studentgrades
```

```{r loading studentgrades using most recent grade, eval=TRUE}
load("StudentGrades.RData")
head(studentgrades)
```



## Using Professor data
### Cleaning process
Since all professors for each course are saved in individual csvs by subject and term we need to compile all professor data into one data frame. We save the resulting into a rdata file.
```{r compiling prof csv data, eval=FALSE}
# getting all file names in UBCO-Profs folder
file_names <- dir(path = "UBCO-Profs")

# full prof dataframe
course_profs <- data.frame(Professor=character(), year_code=character(), course_name=character(), Section=character())

for(i in 1:length(file_names)){
  # Try to open the file connection
  file_connection <- try(read.csv(paste0("UBCO-Profs/", file_names[i])), silent = TRUE)

  # Check if the connection was successful
  if (inherits(file_connection, "try-error")) {
    
  } else {
    df <- file_connection
    df$course_name <- paste(file_connection$Subject, file_connection$Course, sep = ".")
    df$year_code <- paste0(file_connection$Year, file_connection$Session)
    df <- df[,c("Professor", "year_code", "course_name", "Section")]
    
    course_profs <- rbind(course_profs, df)
  }

}

# both of these have no prof names
course_profs <- course_profs[course_profs$Section != "OVERALL",]
course_profs <- course_profs[course_profs$Professor != "",]

save(course_profs, file = "course_profs.RData")
```

However, this data is messy and has several names in the professor column. My assumption is that TAs and co professors are added to the list along with the main professor. This chunk is used in the cleaning process and finds out how many times each unique professor name appears in the data frame.
```{r adjusting prof data table, eval=FALSE}
# all unique 'professor' names
prof_names_full_data <- course_profs$Professor
# length
len <- length(prof_names_full_data)

# empty vectors
prof_names_all <- character()
prof_lens <- numeric()

# looping through to fill two vectors
for(i in 1:len){
  # adding all names to a pile of names (yes duplicates)
  prof_names_all <-  c(prof_names_all, strsplit(prof_names_full_data[i], ";")[[1]])
  # adding each unique 'professor' name's length (ie. # of profs)
  prof_lens <- c(prof_lens, length(strsplit(prof_names_full_data[i], ";")[[1]]))
}

# DF of all 'prof' names with more than one person
# course_profs[which(prof_lens > 1),]


# creating a table count of # of times profs taught
prof_table <- table(prof_names_all)
ordered_profs <- sort(prof_table, decreasing = TRUE)
```


We need to reduce the number of names in each column to only a single name in order to standardize and factor professors. While rudimentary, the one name we take is the name which has appeared the most times in the entire data frame. The logic behind this is that the names with the most occurrences over the years will be professors as opposed to students who TA for 2 years. We don't count only names which are by themselves (no TAs etc.) to find professors with for one reason. Many subjects only list professors with their TAs resulting in many professors being excluded from the table which counts name occurrences. This messes with the professor of many courses as no professor is found in the name occurrences table.

Yes there are better solutions, however as a test to see if professor data would improve the model, this is the algorithm I chose. We could change this to save multiple professors if both appear on their own. One problem I foresee with this is when the main prof never appears on their own, but the co prof does resulting in only the co professor being saved.
```{r making table of courses and their professors, eval=FALSE}
# making a new table to join on (to student grades)
rel_course_profs <- course_profs
ordered_prof_names <- rownames(ordered_profs)


# loop through all rows
for(r in 1:nrow(rel_course_profs)){
  
  # if professor has more than one prof
  if(length(strsplit(rel_course_profs[r,"Professor"],";")[[1]]) > 1){
    
    # loop through prof names to save first one which appears
    for(i in 1:length(ordered_prof_names)){
      
      # checking if current ordered prof is in list of profs
      if(ordered_prof_names[i] %in% strsplit(rel_course_profs[r,"Professor"],";")[[1]]){
          rel_course_profs[r,"Professor"] <- ordered_prof_names[i]
          break
      }
      
    }
  }
    
}


save(rel_course_profs, file = "cleaned_profs.RData")
```

```{r loading professor cleaning and final data, eval=TRUE}
# course_profs
load("course_profs.RData")
# rel_course_profs
load("cleaned_profs.RData")
```

The data frame rel_course_profs contains rows containing the Professor, Year, Course name, and Section for each course and section from 2018S to 2023S.


### Professor-student data structure
This was a temporary chunk used to create a data frame to test the model with. courses was saved to a short list of 8 courses which was used to initially test with GBMs
```{r adding profs to studentgrades *OLD*, eval=FALSE}
courses <- unique(rel_course_profs$course_name)
courses <- colnames(studentgrades[,colnames(studentgrades) %in% courses])

courseprof_cols <- c(courses, paste0(courses,".P"))


# cbind(studentgrades[,c(1:5)],studentgrades[,colnames(studentgrades) %in% courses])

# cutting out all courses which will cause prof error
df_bsc_relevant_courses <- df_bsc[df_bsc$COURSE_CODE %in% courses,]

# adding in professors
matching_profs <- character()
for(i in 1:nrow(df_bsc_relevant_courses)){
  if(length(rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor) == 0){
    matching_profs <- c(matching_profs, NA)
    # next
    
  } else {
    matching_profs <- c(matching_profs, rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor)
  }
}

rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor


length(matching_profs)
nrow(df_bsc_relevant_courses)

# creating list of unique ids
uniqueids <- unique(df_bsc_relevant_courses$STUD_NO_ANONYMOUS)

### from another chunk
studentgrades_prof <- data.frame(Student_ID = sort(uniqueids))

# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(studentgrades_prof), ncol = length(courseprof_cols)))
colnames(new_columns) <- courseprof_cols

# Add course name columns to existing data frame
studentgrades_prof <- cbind(studentgrades_prof, student_mm[,-1], new_columns)


prof_column_adjust <- length(courses)

# Saving student grades into df
for (i in 1:nrow(df_bsc_relevant_courses)){
  
  ## we have two 36,000 length things, df_bsc_rel_courses and matching_profs
  ## need to pull grades from df and profs from match_profs ROW BY ROW
  ## add to studengrades_prof, can split using + length(courses) between course and prof columns
  
  # finding the index of course name .1
  coursecol <- grep(df_bsc_relevant_courses[i,]$COURSE_CODE, colnames(studentgrades_prof))[1]
  
  # Saving student grade into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol] <- df_bsc_relevant_courses[i,]$HDR_CRS_PCT_GRD
  
  # saving prof name into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+prof_column_adjust] <- matching_profs[i]
}

studentgrades_prof[,880:890]
```

This chunk is used to make a new studentgrades_prof data frame with both courses and their respective professors as columns. By looping through each row in df_bsc_relevant_columns, the relevant data is added to studentgrades_prof.

The data frame studentgrades_prof is similar to studentgrades (most recent grades version) in that students grades are saved as rows, and each columns is a different course. Additionally, there are columns which contain the professors who taught each course there is a grade for. For example, Student X has a grade in the MATH.101 column and will therefore have a professor in the MATH.101.P column. Student Y who has not taken MATH 101 will not have a grade in MATH.101, nor a professor in MATH.101.P.
```{r creating studentgrades_prof, eval=FALSE}
# creating all columns to be used in studentgrades_prof
# course_names from 'finding number of times courses were taken' chunk
courseprof_cols <- c(course_names, paste0(course_names,".P"))


# creating list of unique ids
uniqueids <- unique(df_bsc_relevant_courses$STUD_NO_ANONYMOUS)

### from another chunk
studentgrades_prof <- data.frame(Student_ID = sort(uniqueids))

# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(studentgrades_prof), ncol = length(courseprof_cols)))
colnames(new_columns) <- courseprof_cols

# Add course name columns to existing data frame
studentgrades_prof <- cbind(studentgrades_prof, student_mm[,-1], new_columns)


prof_column_adjust <- length(course_names)

# Saving student grades into df
for (i in 1:nrow(df_bsc_relevant_courses)){
  
  
  # finding the index of course name
  coursecol <- grep(df_bsc_relevant_courses[i,]$COURSE_CODE, colnames(studentgrades_prof))[1]
  
  # Saving student grade into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol] <- df_bsc_relevant_courses[i,]$HDR_CRS_PCT_GRD
  
  # catching if there is no prof for course
  if(length(rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor) == 1){
    
    
    # saving prof name into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+prof_column_adjust] <- rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor
  }
  
  # catching if there are more than one prof showed (PSYO 380 since it's missing 'Detail' in main student grades csv)
  if(length(rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor) > 1){
    
    
    # saving prof name into studentgrades_prof df
  studentgrades_prof[studentgrades_prof$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+prof_column_adjust] <- rel_course_profs[rel_course_profs$year_code == df_bsc_relevant_courses[i,"YEAR_CODE"] & 
                   rel_course_profs$course_name == df_bsc_relevant_courses[i,"COURSE_CODE"] & 
                   rel_course_profs$Section == df_bsc_relevant_courses[i,"SEC_NO"],]$Professor[1]
  }
  
  
}

studentgrades_prof


# removing columns with no or one prof
cols_to_remove <- numeric(0)
for(i in (prof_column_adjust+6):ncol(studentgrades_prof)){
  if(length(unique(studentgrades_prof[,i])) <= 2){
    cols_to_remove <- c(cols_to_remove, i)
  }
}

studentgrades_prof <- studentgrades_prof[,-cols_to_remove]


# factoring professor columns
for(i in (prof_column_adjust+6):ncol(studentgrades_prof)){
  studentgrades_prof[,i] <- factor(studentgrades_prof[,i])
}

save(studentgrades_prof, file="Prof_StudentGrades.RData")
```

```{r loading studentgrades_prof, eval=TRUE}
load(file="Prof_StudentGrades.RData")
head(studentgrades_prof)
```






# Analysis and Prediction

Throughout my time on this project I have switched which data frame I used to train and test each model and method. The three data frames are described above but are called studentgrades_rep, studentgrades, and studentgrades_prof respectively. For some of the main methods using 'old' data frames I will use other data frames for comparison.

## Initial EDA

These are the first plots I made in order to get a feel for the data. They do not show much insight into the data.
```{r plot of MATH.100.1 against MATH.100.2,eval=FALSE}
# Big ol graph of student grades
maths <- studentgrades_rep[complete.cases(studentgrades_rep$MATH.100.1, studentgrades_rep$MATH.100.2), ]


maths.lm <- lm(maths$MATH.100.1~ maths$MATH.100.2)
summary(maths.lm)

plot(maths$MATH.100.1, maths$MATH.100.2)
abline(maths.lm)

## weird....
```

```{r plot of MATH.100.1 against STAT.230.1, eval=FALSE}
mathstat <- studentgrades_rep[complete.cases(studentgrades_rep$MATH.100.1, studentgrades_rep$STAT.230.1), ]

mathstat.lm <- lm(mathstat$MATH.100.1~ mathstat$STAT.230.1)
summary(mathstat.lm)

plot(mathstat$MATH.100.1, mathstat$STAT.230.1)
abline(mathstat.lm)

## this one is a little more normal
## could be issues with course retakers

## want to check on people who only had to take it once...
```

```{r plot of MATH.100.1 against PSYO.111.1, eval=FALSE}
edasubset <- studentgrades_rep[complete.cases(studentgrades_rep$MATH.100.1, studentgrades_rep$PSYO.111.1), ]

edasubset.lm <- lm(edasubset$MATH.100.1~ edasubset$PSYO.111.1)
summary(edasubset.lm)

plot(edasubset$MATH.100.1, edasubset$PSYO.111.1)
abline(edasubset.lm)

## ... makes sense that some courses like PSYO 111 are grade boosters
```

```{r plot of MATH.100.2 against STAT.230.1, eval=FALSE}
mathstat2 <- studentgrades_rep[complete.cases(studentgrades_rep$MATH.100.2, studentgrades_rep$STAT.230.1), ]

mathstat2.lm <- lm(mathstat2$MATH.100.2~ mathstat2$STAT.230.1)
summary(mathstat2.lm)

plot(mathstat2$MATH.100.2, mathstat2$STAT.230.1)
abline(mathstat2.lm)
```

```{r plot of MATH.100.1 (no MATH.100.2 grade present) againt STAT.230.1, eval=FALSE}

## want to check on people who only had to take it once (aka no MATH.100.2 grade present)

firsttimer <- anti_join(mathstat,mathstat2)
first.lm <- lm(firsttimer$MATH.100.1~ firsttimer$STAT.230.1)
summary(first.lm)

plot(firsttimer$MATH.100.1, firsttimer$STAT.230.1, xlim = c(0,100))
abline(first.lm)

# just removed the 'low' grades (p much all fails) 
```

This chunk uses studentgrades_rep and makes a data frame including each course and its average grade. Doesn't quite work right now...
```{r finding average grades for each course, eval=FALSE}
# want the average grade for each course... bar plot
course_averages <- data.frame(Courses = course_names_dups, Average = rep(NA, length(course_names_dups)))

for (i in 1:nrow(course_averages)){
  course_averages[i,]$Average <- mean(studentgrades_rep[,course_averages[i,1]],na.rm = TRUE)
}

# barplot of all course average grades
barplot(course_averages$Average, names.arg = course_averages$Courses)

# average grades for each course
head(course_averages)
```

This chunk uses studentgrades_rep to make a pairplot of MATH.100.1, MATH.101.1, STAT.230.1, DATA.101.1.
```{r pairplot of MATH.100.1, MATH.101.1, STAT.230.1, DATA.101.1, eval=FALSE}
# pairplot using data which has no missing grades from these courses
plot(studentgrades_rep[!is.na(studentgrades_rep$MATH.100.1)&!is.na(studentgrades_rep$MATH.101.1)&!is.na(studentgrades_rep$STAT.230.1)&!is.na(studentgrades_rep$DATA.101.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","DATA.101.1")])

# studentgrades data which has no missing STAT.230 grades
studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")]

# pairplot using data which has no missing STAT.230 grades
plot(studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")])
```

```{r plot of MATH.100.1 against MATH.101.1, eval=FALSE}
plot(studentgrades_rep[!is.na(studentgrades_rep$MATH.100.1)&!is.na(studentgrades_rep$MATH.101.1),c("MATH.100.1","MATH.101.1")])
```

This chunk returns the number of times a course has been taken by students WHO HAVE ALL TAKEN stat 230. Used to view variables which may be useful. This code is used in a later chunk.
```{r number of times courses have been taken by those who have taked STAT 230, eval=FALSE}
# returns the number of times a course has been taken by students WHO HAVE ALL TAKEN stat 230
ids <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$COURSE_CODE=="STAT.230",]$STUD_NO_ANONYMOUS)
courses <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids,]$COURSE_CODE)
new_courses <- character(0)
for(i in 1:length(courses)){
  if(strsplit(courses[i], split = ".", fixed = TRUE)[[1]][2] < 300){
    new_courses <- append(new_courses, courses[i])
  }
}

courses_taken_bsc_stat230 <- table(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids & df_bsc_relevant_courses$COURSE_CODE %in% new_courses,]$COURSE_CODE)


sorted_courses_taken_bsc <- sort(courses_taken_bsc_stat230, decreasing = TRUE)
sorted_courses_taken_bsc

# number of courses who have been taken by at least N people: 25 people = 105 courses | 250 people = 25 courses
N = 25
length(sorted_courses_taken_bsc[sorted_courses_taken_bsc >= N])


```

## First Prediction Attempts

These first attempts were made on either studentgrades_rep or studentgrades. Attempts made using studentgrades_prof were added in after for comparison.

### MissForest
The MissForest package is used to impute missing data. After using it to impute NA's into student grades, we can use the imputed data set with other models which don't allow for missing data/NAs. One of this packages use cases was imputing the data and using it in a Random Forest model. Since imputation is computationally expensive, we only impute on a small number of courses.
This chunk uses studentgrades_rep and only uses each selected courses first attempt to impute and predict.
```{r missForest Impute into RF on course first attempts, eval=FALSE}
# preparing all student grades with stat230
stat230grades <- studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","MATH.200.1","MATH.221.1","COSC.211.1","COSC.222.1","COSC.221.1")]
stat230grades

library(missForest)
library(randomForest)

n_rows <- nrow(stat230grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,2:5])$ximp

stat230.rf <- randomForest(STAT.230.1~., data=stat230.imputed[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- stat230grades[test_indices,]$STAT.230.1

# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
rmse
```

This chunk uses studentgrades with the same few selected courses. While this is one very good result I have tested on a few other seeds and it generally performs quite well. With some CV or other methods this could work well.
```{r missForest Impute into RF using few courses (stats maths cosc), eval=TRUE}
set.seed(5934)
# preparing all student grades with stat230
stat230grades <- studentgrades[!is.na(studentgrades$STAT.230),c("Student_ID","MATH.100","MATH.101","STAT.230","MATH.200","MATH.221","COSC.211","COSC.222","COSC.221")]
# stat230grades

library(missForest)
library(randomForest)

n_rows <- nrow(stat230grades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,2:9])$ximp

stat230.rf <- randomForest(STAT.230~., data=stat230.imputed[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- stat230grades[test_indices,]$STAT.230

# RMSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
rmse

# graphs
## predicted vs actual plot
predictions <- stat230.pred
actual <- stat230.real


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))
abline(0, 1, col = "red")

line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")


## residuals plot
residuals <- actual - predictions
plot(predictions, residuals, xlab = "Predicted Grades", ylab = "Residuals", 
     main = "Residual Plot", pch = 16, col = "blue")
abline(h = 0, col = "red")

line.lm <- lm(residuals~predictions)
abline(line.lm, col = "green")


## qq plots
qqnorm(residuals, main = "Normal Q-Q Plot of Residuals")
qqline(residuals, col = "red")
```

In this chunk I wanted to try a method that worked before with the new studentgrades_prof data. As we can see it still works even when imputing professor data.
```{r missForest Impute into RF on studentgrades_prof, eval=TRUE}
set.seed(5934)
# preparing all student grades with stat230
stat230grades <- studentgrades_prof[!is.na(studentgrades_prof$STAT.230),c("MATH.100","MATH.101","STAT.230","MATH.200","MATH.221","COSC.211","COSC.222","COSC.221","MATH.100.P","MATH.101.P","STAT.230.P","MATH.200.P","MATH.221.P","COSC.211.P","COSC.222.P","COSC.221.P")]
# stat230grades

library(missForest)
library(randomForest)

n_rows <- nrow(stat230grades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades)$ximp

stat230.rf <- randomForest(STAT.230~., data=stat230.imputed[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- stat230grades[test_indices,]$STAT.230

# RMSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
rmse

# graphs
## predicted vs actual plot
predictions <- stat230.pred
actual <- stat230.real


plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))
abline(0, 1, col = "red")

line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")

```

In this chunk I cut right to the chase and use the imputed values as grade predictions. As expected it doesn't work very well.
```{r Using missForest for prediction, eval=TRUE}
set.seed(5934)
# preparing all student grades with stat230
stat230grades <- studentgrades[!is.na(studentgrades$STAT.230),c("MATH.100","MATH.101","STAT.230","MATH.200","MATH.221","COSC.211","COSC.222","COSC.221")]
stat230grades

library(missForest)
library(randomForest)

n_rows <- nrow(stat230grades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

# to remove stat 230 grades from test_indicies rows
stat230.real <- stat230grades[test_indices,]$STAT.230
stat230grades[test_indices,]$STAT.230 <- NA

stat230.imputed <- missForest(stat230grades)$ximp


stat230.pred <- stat230.imputed[test_indices,]$STAT.230


# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
rmse
```


### Random Forest
In this section I've added the other ways I implemented random forests since many attempts were included above since missForest was used in order to keep as many rows as possible.

In this chunk I take the complete cases of MATH.100.1, MATH.101.1, STAT.230.1, and DATA.101.1 to train a RF model and predict STAT 230 grades. Note that this is using studentgrades_rep.
```{r RF on all non-NA data, eval=TRUE}
# getting all students with grades in all courses
stat230grades.full <- studentgrades_rep[!is.na(studentgrades_rep$MATH.100.1)&!is.na(studentgrades_rep$MATH.101.1)&!is.na(studentgrades_rep$STAT.230.1)&!is.na(studentgrades_rep$DATA.101.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","DATA.101.1")]


n_rows <- nrow(stat230grades.full)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(randomForest)

stat230.rf <- randomForest(STAT.230.1~., data=stat230grades.full[train_indices,2:5] )
stat230.pred <- predict(stat230.rf, newdata = stat230grades.full[test_indices,2:5])

stat230.real <- stat230grades.full[test_indices,2:5]$STAT.230.1

# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
rmse

## MSE of data without DATA101 is 175...
## looks like imputing data and running a randomforest is best so far
```

This chunk is a repeat of the previous one, except using studentgrades. Neither seem to produce decent predictions, however, I believe there is not enough data to make a good model.
```{r RF on complete data, eval=TRUE, warning=FALSE}
# this chunk uses a random forest model trained on the complete set of STAT 230, MATH 101, and COSC.221 (aka no NAs between them)
coursegrades <- studentgrades[!is.na(studentgrades$STAT.230)&!is.na(studentgrades$MATH.101)&!is.na(studentgrades$MATH.100)&!is.na(studentgrades$DATA.101),c("STAT.230","MATH.101","MATH.100","DATA.101")]

set.seed(5934)

library(randomForest)

n_rows <- nrow(coursegrades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.rf <- randomForest(STAT.230~., data=coursegrades[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = coursegrades[test_indices,])



# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
rmse
```

This chunk implements missForest and Random Forests to predict grades. However, only courses which have been taken a certain number of times (times_taken = 190) are used as predictors during imputation, training, and testing. This is a rudimentary variable selection method but ensures we use predictors with the most existing data.
```{r missForest Impute into RF on all courses taken x times, eval=TRUE}
# returns the number of times a course has been taken by students who have also taken stat 230
ids <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$COURSE_CODE=="STAT.230",]$STUD_NO_ANONYMOUS)
courses <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids,]$COURSE_CODE)

# this is used to filter out course predictors (columns) which are from years above the course we're predicting on (STAT.230)
new_courses <- character(0)
for(i in 1:length(courses)){
  if(strsplit(courses[i], split = ".", fixed = TRUE)[[1]][2] < 300){
    new_courses <- append(new_courses, courses[i])
  }
}

courses_taken_bsc_stat230 <- table(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids & df_bsc_relevant_courses$COURSE_CODE %in% new_courses,]$COURSE_CODE)


sorted_courses_taken_bsc <- sort(courses_taken_bsc_stat230, decreasing = TRUE)

##
## Filter course names used base on how many people have taken those predictor courses
##
times_taken <- 190
course_names <- sorted_courses_taken_bsc[sorted_courses_taken_bsc >= times_taken]


course_names <- dimnames(course_names)[[1]]


coursegrades <- studentgrades[!is.na(studentgrades$STAT.230),colnames(studentgrades) %in% course_names]
head(coursegrades)

library(missForest)
library(randomForest)

set.seed(5934)
n_rows <- nrow(coursegrades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(coursegrades[,-1])$ximp

stat230.rf <- randomForest(STAT.230~., data=stat230.imputed[train_indices,])
predictions <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- coursegrades[test_indices,]$STAT.230

# RMSE value to compare to other models
rmse <- sqrt(mean((predictions - stat230.real)^2))
rmse
```

This is a repeat of the last chunk with modifications which make it easier to use different courses as the course we predict for.
```{r missForest Impute into RF on all courses *USED FOR TESTING ON DIFF COURSES* (COSC.221), eval=FALSE}
# returns the number of times a course has been taken by students who have also taken xxxx.xxx
ids <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$COURSE_CODE=="COSC.221",]$STUD_NO_ANONYMOUS)
courses <- unique(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids,]$COURSE_CODE)
new_courses <- character(0)
for(i in 1:length(courses)){
  if(strsplit(courses[i], split = ".", fixed = TRUE)[[1]][2] < 300){
    new_courses <- append(new_courses, courses[i])
  }
}

courses_taken_bsc_stat230 <- table(df_bsc_relevant_courses[df_bsc_relevant_courses$STUD_NO_ANONYMOUS %in% ids & df_bsc_relevant_courses$COURSE_CODE %in% new_courses,]$COURSE_CODE)


sorted_courses_taken_bsc <- sort(courses_taken_bsc_stat230, decreasing = TRUE)
times_taken <- 50
course_names <- sorted_courses_taken_bsc[sorted_courses_taken_bsc >= times_taken]



set.seed(45)

# of the same year and below
# preparing all student grades with stat230
course_names <- dimnames(course_names)[[1]]



# course_names <- c("COSC.121","MATH.200","MATH.221","COSC.221","COSC.211","MATH.101","COSC.222","ECON.101","COSC.111","PSYO.111","STAT.230")

stat230grades <- studentgrades[!is.na(studentgrades$COSC.221),colnames(studentgrades) %in% course_names]
head(stat230grades)

library(missForest)
library(randomForest)

n_rows <- nrow(stat230grades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,-1])$ximp

stat230.rf <- randomForest(COSC.221~., data=stat230.imputed[train_indices,])
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- stat230grades[test_indices,]$COSC.221

# MSE value to compare to other models
rmse <- sqrt(mean((stat230.pred - stat230.real)^2))
rmse

```

### Gradient Boosted Machines
This section shows my initial attempts at gradient boosted machines. The first chunk uses studentgrades_rep to train and test on.
```{r GBM on partial-NA data, eval=TRUE}
# loading in data again
stat230grades <- studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")]
n_rows <- nrow(stat230grades)

set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 6000,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.01,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230.1 ~ ., data = stat230grades[train_indices,2:5], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new data frame for prediction
predictions <- predict(gbm_model, newdata = stat230grades[test_indices,2:5], n.trees = gbm_params$n.trees)

# Print the predictions
# print(predictions)

rmse <- sqrt(mean((predictions - stat230grades[test_indices,2:5]$STAT.230.1)^2))
rmse
## 175 mse... getting worse...
```

This chunk uses the complete cases of MATH.100.1, MATH.101.1, STAT.230.1, and DATA.101.1 from studentgrades_rep to train and test. 
```{r GBM on non-NA data, eval=TRUE}
# loading in data again TRYING WITH FULL DATA
stat230grades <- studentgrades_rep[!is.na(studentgrades_rep$MATH.100.1)&!is.na(studentgrades_rep$MATH.101.1)&!is.na(studentgrades_rep$STAT.230.1)&!is.na(studentgrades_rep$DATA.101.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","DATA.101.1")]
# stat230grades
n_rows <- nrow(stat230grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.005,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230.1 ~ ., data = stat230grades[train_indices,2:5], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new data frame for prediction
predictions <- predict(gbm_model, newdata = stat230grades[test_indices,2:5], n.trees = gbm_params$n.trees)

# Print the predictions
# print(predictions)

rmse <- sqrt(mean((predictions - stat230grades[test_indices,2:5]$STAT.230.1)^2))
rmse
## 128 mse which is a little better when you've got full data
```

To cover all bases and check if it may be better, we impute grades onto the same courses and train a gbm model. The results are not great but quite a bit better than before.
```{r missForest impute into GBM, eval=TRUE}
# now, does imputing the data before GBM work better?
# preparing all student grades with stat230
stat230grades <- studentgrades_rep[!is.na(studentgrades_rep$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")]
# stat230grades

library(missForest)
library(gbm)

n_rows <- nrow(stat230grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,2:5])$ximp

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.005,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230.1 ~ ., data = stat230.imputed[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
# print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = stat230.imputed[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
# print(predictions)

rmse <- sqrt(mean((predictions - stat230.imputed[test_indices,]$STAT.230.1)^2))
rmse
## 109 mse.. better still
```

This chunk will use students grades of the current year (eg. second year uses STAT.230 and COSC.221) to train the GBM model. However, the test set will have grades from the current year removed (eg. won't use COSC.221 as a predictor). In hindsight this isn't a great idea when using GBMs as the decision trees used won't be configured for the subset of courses used. Also to note, this uses studentgrades_rep.
```{r, eval=FALSE}
## sets all courses of equal or higher level to NA before prediction, but after training the model
set.seed(5934)
# time to remove all future year courses from prediction
COI <- "STAT.230.1"
coursegrades <- studentgrades_rep[!is.na(studentgrades_rep[,COI]),-1:-5]

# figuring out the year level of each course...
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- column_names[column_digits <= YOI]
cols_to_keep <- append(cols_to_keep, COI)

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# extras/ to do next
grade_counts <- colSums(!is.na(coursegrades))
# Subset columns with less than 10 grades
sparse_cols <- names(grade_counts[grade_counts <= 5])

# Need to exclude future attempts at the course, but not past
## eg. PSYO.380.1 shouldn't use 380.2 as a predictor, but 380.2 can use 380.1
if(length(strsplit(COI, split = ".", fixed = TRUE)[[1]]) == 3){
  cname <- strsplit(COI, split = ".", fixed = TRUE)[[1]][1]
  cnum <- strsplit(COI, split = ".", fixed = TRUE)[[1]][2]
  crep <- as.integer(strsplit(COI, split = ".", fixed = TRUE)[[1]][3])
  # remove all future instances of course (if they exist)
  for(i in 1:length(column_names)){
    if(strsplit(column_names[i], split = ".", fixed = TRUE)[[1]][1] == cname & strsplit(column_names[i], split = ".", fixed = TRUE)[[1]][2] == cnum & as.integer(strsplit(column_names[i], split = ".", fixed = TRUE)[[1]][3]) > crep){
      sparse_cols <- append(sparse_cols, column_names[i])
    }
  }
}

# Subset the dataframe to include only courses used to predict on
coursegrades <- coursegrades[, -which(names(coursegrades) %in% sparse_cols)]

set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

cols_to_na <- column_names[column_digits == YOI] #### ADD BACK COURSE OF INTEREST!!!!!!!
cols_to_na <- cols_to_na[!cols_to_na == COI]
coursegrades[test_indices, which(names(coursegrades) %in% cols_to_na)] <- NA

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 100,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.1,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)



gbm_model <- gbm(STAT.230.1 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
predictions

rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
rmse

```

### Matrix Factorization

Since the studentgrades data frame is highly dimensional I wanted to see if matrix factorization would work well on this dataset. The results were decent, but no grid search was implemented to further improve this method.
```{r Dataset creation for MF, eval=TRUE}
# Getting list of students wanted
studs <- studentgrades[!is.na(studentgrades$STAT.230),]$Student_ID

data <- data.frame(student = character(), course=character(), grade=double())
for(id in studs){
  for(c in 6:205){
    if(!is.na(studentgrades[studentgrades$Student_ID==id,c])){
      data <- rbind(data, list(id,colnames(studentgrades)[c],studentgrades[studentgrades$Student_ID==id,c]))
    }
  }
}

colnames(data) <- c("student","course","grade")

data <- na.omit(data)  # Remove rows with NA ratings

# Split into obj course and not
data_obj <- data[data$course=="STAT.230",]
data_non <- data[!data$course=="STAT.230",]

# Creating the testing set from obj course
n_rows <- nrow(data_obj)

train_indices <- sample(1:n_rows, 0.9 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

data <- rbind(data_obj[test_indices,], data_obj[train_indices,],data_non)


# Factoring data
data[] <- lapply(data, function(x) {
  if (is.character(x)) {
    as.numeric(factor(x))
  } else {
    x
  }
})

obj_course <- data[1,2]
```

```{r Matrix Factorization prediction, eval=TRUE}
library(recosystem)

# Create a Reco object
r <- Reco()

# Define the training set
set.seed(5934)
train_set <- data_memory(user_index = data$student[-1:-length(test_indices)], item_index = data$course[-1:-length(test_indices)], rating = data$grade[-1:-length(test_indices)])

# Train the model with ALS
r$train(train_set, opts = list(dim = 10, costp_l2 = 0.2, costq_l2 = 0.2, lrate = 0.05, niter = 20))


# Initialize the matrix to store predicted grades
predicted_grades <- data[1:length(test_indices),]
predicted_grades$grade <- NA

# Predict missing values

for(i in 1:nrow(predicted_grades)){
  predicted_grades[i,]$grade <- r$predict(data_memory(user_index = predicted_grades[i,]$student, item_index = predicted_grades[i,]$course))
}


# Print the predicted grades matrix
# print(predicted_grades)


# Calculate MSE for observed (non-NA) grades
observed_indices <- which(!is.na(grades))
rmse <- sqrt(mean((predicted_grades$grade - data_obj[test_indices,]$grade)^2))
rmse
```

### Structured Learning

```{r Structured Learning, eval=FALSE}

library(bnlearn)

suppressWarnings({
pc <- pc.stable(studentgrades[,-(1:5)], undirected = FALSE)
pc$arcs # 10 arcs

gs <- gs(studentgrades[,-(1:5)], undirected = FALSE)
gs$arcs # 10 arcs

iamb <- iamb(studentgrades[,-(1:5)])
iamb$arcs # 75 arc... some potential

hc <- tabu(studentgrades[,-(1:5)], score = "aic")
hc$arcs

mmhc <- h2pc(studentgrades[,-(1:5)])
mmhc$arcs

mmpc <- mmpc(studentgrades[,-(1:5)])
mmpc$arcs # lots of pairs but no substance
})

chow <- chow.liu(studentgrades[,-(1:5)])
chow$arcs[chow$arcs[,1]=="STAT.230"]

mmpc$arcs[mmpc$arcs[,1]=="STAT.230"]
```

### Support Vector Machines
Many research papers online have said they get one of the best prediction from SVMs so I figured I had to try it out. Using the base package e1071 support vector machines were implemented, however the predictions were not as good as we imagined.
```{r SVM model, eval=FALSE}
library(e1071)
library(missForest)

course_names <- c("COSC.121","MATH.200","MATH.221","COSC.221","COSC.211","MATH.101","COSC.222","ECON.101","COSC.111","PSYO.111","STAT.230")

grades <- studentgrades[!is.na(studentgrades$STAT.230),colnames(studentgrades) %in% course_names]

#grades.imputed <- missForest(grades)$ximp


n_rows <- nrow(grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

xtrain <- grades[train_indices,!names(grades) %in% c("STAT.230")]
ytrain <- grades[train_indices,"STAT.230"]
xtest <- grades[test_indices,!names(grades) %in% c("STAT.230")]
ytest <- grades[test_indices,"STAT.230"]


svm_model <- svm(STAT.230~., data=grades[train_indices,])

#opt_svm <- tune(svm_model, STAT.230~., data=grades[train_indices,],ranges=list(elsilon=seq(0,1,0.1), cost=1:100))

pred_svm <- predict(svm_model, data=grades[test_indices[1],])
length(pred_svm)
pred_svm

grades.real <- grades[test_indices,"STAT.230"]

rmse <- sqrt(mean((pred_svm - grades.real)^2))
rmse
```

We figured that with traincontrol and some hyperparameter tuning we would get some better results. Fortunately we did get good results, including an rmse on par with GBMs. This was not initially tried with the professor data and may be worth revisiting.
```{r SVM via trainControl, eval=TRUE}
library(e1071)
library(missForest)
library(kernlab)  # install if necessary using 'install.packages'
library(caret) # install if necessary using 'install.packages'

course_names <- c("COSC.121","MATH.200","MATH.221","COSC.221","COSC.211","MATH.101","COSC.222","ECON.101","COSC.111","PSYO.111","STAT.230")

grades <- studentgrades[!is.na(studentgrades$STAT.230),colnames(studentgrades) %in% course_names]
#grades <- studentgrades[!is.na(studentgrades$STAT.230),]

grades <- missForest(grades)$ximp


n_rows <- nrow(grades)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

xtrain <- grades[train_indices,!names(grades) %in% c("STAT.230")]
ytrain <- grades[train_indices,"STAT.230"]
xtest <- grades[test_indices,!names(grades) %in% c("STAT.230")]
ytest <- grades[test_indices,"STAT.230"]


#To train the model we will use k-fold cross validation, with k set to 5
ctrl <- trainControl(method = "cv", number=5) 


# Create a grid of parameters to test and train the model with dimension 1
SVRGridCoarse <- expand.grid(.sigma=c(0.001, 0.01, 0.1), .C=c(10,100,1000))
SVRFitCoarse <- train(xtrain, ytrain, method="svmRadial", tuneGrid=SVRGridCoarse, trControl=ctrl, type="eps-svr")

# Display results
SVRFitCoarse

ggplot(SVRFitCoarse)

SVRFitCoarse$finalModel

yPred <- predict(SVRFitCoarse$finalModel, xtest)
```

### Recursive Feature Elimination
This chunk was meant to be an attempt on using recursive feature elimination. However, this method doesn't work for data with NA values and couldn't be implemented with studentgrades.
```{r Trying RFE... doesnt work on NA, eval=FALSE}
library("dplyr")
library("faux")
library("DataExplorer")
library("caret")
library("randomForest")

studentgrades.factor <- studentgrades %>%
  # Save categorical features as factors
  mutate_at(c("Student_ID", "Major.1", "Major.2", "Minor", "Honors"), 
            as.factor)


# Define the control using a random forest selection function
control <- rfeControl(functions = rfFuncs, # random forest
                      method = "repeatedcv", # repeated cv
                      repeats = 5, # number of repeats
                      number = 10) # number of folds

n_rows <- nrow(studentgrades.factor)
set.seed(5934)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

xtrain <- studentgrades.factor[train_indices,!names(studentgrades.factor) %in% c("STAT.230")]
ytrain <- studentgrades.factor[train_indices,"STAT.230"]
xtest <- studentgrades.factor[test_indices,!names(studentgrades.factor) %in% c("STAT.230")]
ytest <- studentgrades.factor[test_indices,"STAT.230"]

# Run RFE
result_rfe1 <- rfe(x = xtrain, 
                   y = ytrain, 
                   sizes = c(1:13),
                   rfeControl = control)

# Print the results
result_rfe1

# Print the selected features
predictors(result_rfe1)

# Print the results visually
ggplot(data = result_rfe1, metric = "Accuracy") + theme_bw()
ggplot(data = result_rfe1, metric = "Kappa") + theme_bw()
```

### LASSO Feature Selection
In this section there are two incomplete chunks of code. I had moved off of working on feature selection as we deemed GBMs were doing a good job at indicating which courses are the most important. 
lasso regression doesn't handle NA values so I tried to avoid that by using na.action = na.exclude. However, that didn't work.
```{r LASSO feature selection, eval=TRUE}
cv_5 <- trainControl(method="cv", number=5)
studentgrades_stat230 <- studentgrades[!is.na(studentgrades$STAT.230),]
lasso <- train(STAT.230 ~., data=studentgrades_stat230, method='lasso',  trControl=cv_5, na.action = na.exclude)
```

```{r Lasso.., eval=FALSE}
library(glmnet)

cv_model <- cv.glmnet(studentgrades$STAT.230~studentgrades[,-c(1,2,3,4,5,"STAT.230")], nfolds = 10,alpha=1)
```

### Singular Value Decomposition
I had moved away from trying different modeling methods to focus on improving GBMS.
```{r SVD, eval=FALSE}
library(svd)

trlan.eigen(studentgrades[,-(1:5)])
```




## Further work on GBM
### Summary of the Boosting Process

1. **Initialize the model** with a constant value, typically the mean of the target values for regression:

\[ 
F_0(x) = \arg\min_{c} \sum_{i=1}^{N} L(y_i, c)
\]

2. **For \(m = 1\) to \(M\) (number of boosting iterations)**:
   - Compute the residuals \(r_i^m\)
   - For squared error loss, this simplifies to:
\[
r_i^m = y_i - F_{m-1}(x_i)
\]
   - Fit a base learner \(h_m(x)\) to the residuals.
   - Update the model via gradient decent:

\[
F_{m}(x) = F_{m-1}(x) + \nu h_m(x)
\]

3. **Final prediction** is the sum of all base learners:

\[
\hat{y} = F_M(x) = \sum_{m=1}^{M} \nu h_m(x)
\]


- What we are controlling and changing:
    - Boosting iterations (\texttt{ntrees}): variable...
    - Learning rate (\texttt{shrinkage} parameter): 0.05
    - Loss function distribution: Gaussian


### Data Preperation
To prepare the data fror gradient boosting, I did some cleaning to ensure the data used for training and testing would be available at the time of prediction for the student. For example, a student who wishes to predict their future STAT 230 grade wouldn't typically have 3rd or 4th year courses already completed. And definately wouldn't have grades for any courses with STAT 230 as a prereq.
In this chunk we remove data which doesn't add to the training process. Immediately this is student ID, a students' 2nd major, and whether they're in honors (this could be included but isn't for now). After removing all students who don't have the pre requeset courses, columns (courses) are dropped if they are make up of too many NA values. In our case we want at least 20% of the data to be non-NA. This number was made arbitrarily to reduce the number of courses trained on. The best level of NA values used would be different depending on the course being predicted on. After that, we removed all courses from higher year levels as we wouldn't expect them to be taken.
```{r GBM data prep, eval=TRUE}
# Identifying the course of interest
COI <- "COSC.221"
coursegrades <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 50%
coursegrades <- coursegrades[coursegrades$COSC.221 >= 40, ]
# coursegrades <- coursegrades[coursegrades$COSC.221 <= 80, ]

# factoring student majors
coursegrades$Major.1 <- factor(coursegrades$Major.1)
# factoring student minors
coursegrades$Minor <- factor(coursegrades$Minor)


# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$MATH.101) | !is.na(coursegrades$MATH.103) | !is.na(coursegrades$MATH.142), ]

# coursegrades <- coursegrades[!is.na(coursegrades$MATH.101), ]



# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]





# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- column_names[column_digits <= YOI]

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# display data to be used
head(coursegrades)
```

This chunk is for initializing STAT 230 GBM code as this was the main course I was testing on.
```{r GBM data prep STAT230, eval=TRUE}
# Identifying the course of interest
COI <- "STAT.230"
coursegrades <- studentgrades_prof[!is.na(studentgrades_prof[,COI]),-c(1,3,5)]

# remove all grades < 5%
coursegrades <- coursegrades[coursegrades$STAT.230 >= 5, ]

# factoring student majors
coursegrades$Major.1 <- factor(coursegrades$Major.1)
# factoring student minors
coursegrades$Minor <- factor(coursegrades$Minor)


# removing all students who don't meet course pre reqs DATA 101, COSC 221
coursegrades <- coursegrades[!is.na(coursegrades$DATA.101) | !is.na(coursegrades$COSC.221), ]

# coursegrades <- coursegrades[!is.na(coursegrades$MATH.101), ]



# dropping columns that are very sparse (pct % are NA)
pct <- 0.8
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]





# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- column_names[column_digits <= YOI]

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# display data to be used
head(coursegrades)
```

### Analysis
In this chunk we run the entire data set through a GBM which trains the data in its entirety (there is 10-fold cross validation). From there we can find the performance of the GBM using both cross validation and OOB to display the optimal number of iterations.
```{r finding ideal number of GBM tree graphs to use, eval=FALSE}
library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 10000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(MATH.200 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)


print(gbm.perf(gbm_model, method="cv"))
print(gbm.perf(gbm_model, method="OOB"))
```

The optimal number of iterations is used when training the model to ensure efficiency. CV folds is the only thing which hasn't been strenuesly tested to find the optinal number of folds. Additionally, we print out the model summary, use the model to predict grades, and calculate the rmse of the predicted grades.
```{r GBM model training, eval=TRUE}
set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.005,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(COSC.221 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# saving optimal # of trees
ntrees <- gbm.perf(gbm_model,method="cv")[1]



# Running model a second time with optimal # of trees
gbm_model <- gbm(COSC.221 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

# save real grades
actual <- coursegrades[test_indices,]$COSC.221

rmse <- sqrt(mean((predictions - actual)^2))
rmse
```


### Hyperparameter Training

Here we used trainControl to perform a grid search on the hyperparameters of the GBM algorithm. The main parameters are: n.trees - The number of iterations (decision trees built). interaction.depth - The maximum depth of decision trees. shrinkage - Learning rate (lower rate means it learns less per iteration), if this is too small the model is prone to overfitting. n.minobsinnode - minimum number of observations in terminal nodes. From this we found out a shrinkage of 0.001 or 0.01 is best, a interaction depth of 3-4 is best, the number of trees doesn't matter as we find the optimal # anyways, and n.minobsinnode doesn't have a large impact. Also, train.fraction and bag.fraction of 0.5 are best to ensure some randomness in the model.
```{r gridSearch on GBM, eval=FALSE}
library(caret)
library(tidyverse)

train_control = trainControl(method = "cv", number = 5, search = "grid")

gbmGrid <-  expand.grid(
  # distribution = "gaussian",  # Specify the distribution for regression
  n.trees = c(6000),               # Number of trees (iterations)
  interaction.depth = c(2:4),       # Maximum depth of trees
  shrinkage = c(0.005,0.0025,0.001),           # Learning rate (shrinkage)
  # bag.fraction = 0.5,          # Fraction of training data used for each tree
  # train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = c(5,10)          # Minimum number of observations in terminal nodes
)

# training a Gboost Regression tree model while tuning parameters
model = train(COSC.221~., data = coursegrades, method = "gbm", trControl = train_control, tuneGrid = gbmGrid, na.action = na.pass)

# summarising the results
print(model)

# Make predictions on new data
# predictions <- predict(model$bestTune, newdata = coursegrades[test_indices,], n.trees = ntrees)
# 
# rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
# rmse
```

### Adding Error Bars
This chunk adds error bars onto the graph of predicted vs actual grades. The error bars are calculated by taking the standard deviation of the absolute value of prediction grades - actual grades.
```{r adding error bars, eval=TRUE}
## calculating size of error bars
errors <- abs(predictions - actual)
error_bars <- sd(errors)




## plotting graph and error bars
plot(actual, predictions, 
     xlab = "Actual Grades", ylab = "Predicted Grades",
     main = "Predicted vs Actual Grades with Error Bars", xlim = c(0,100), ylim = c(0,100),pch = 16)

# Add error bars
arrows(actual, predictions - error_bars, 
       actual, predictions + error_bars, 
       angle = 90, code = 3, length = 0.1, col = 1:length(predictions))

abline(0, 1, col = "red")
```

This chunk follows another paper in adding quantile error bars. These are calculated using the "quantile" distribution as a parameter to the gbm. Both upper and lower bounds on the quantile range are found a plotted alongside predictions.
```{r quantile gbm error bars, eval=TRUE}
## training two more gbms using quantile ranges as the prediction interval
library(gbm)

gbm_params <- list(
  distribution = list(name = "quantile", alpha = 0.025),  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(COSC.221 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

LB_predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)


gbm_params <- list(
  distribution = list(name = "quantile", alpha = 0.975),  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(COSC.221 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

UB_predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

## plotting graph and error bars
plot(actual, predictions, 
     xlab = "Actual Grades", ylab = "Predicted Grades",
     main = "Predicted vs Actual Grades with Error Bars", xlim = c(00,100), ylim = c(0,100),pch = 16)

# Add error bars
arrows(actual, LB_predictions, 
       actual, UB_predictions, 
       angle = 90, code = 3, length = 0.1, col = 1:length(predictions))

abline(0, 1, col = "red")
```

Our final way of calculating error bars is by bootstrapping the model 50 times and using the standard deviation to calculate the upper and lower bound for each grade predicted. 
```{r bootstrap gbm error bars, eval=FALSE}
# bootstrapping the model x times
n_iterations = 50  # Number of bootstrapped models
boot_preds = numeric(0)


library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 6000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)


for(i in 1:n_iterations){
  # Create a bootstrapped dataset
  set.seed(100+i)
  
  n_rows <- nrow(coursegrades)
  train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
  test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing
    
  gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

  
  # Predict on test data
  preds <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)
  boot_preds <- append(boot_preds, preds)
  
}

# Convert predictions to a NumPy array
#predictions

# Calculate the mean and standard deviation of the predictions
mean_preds <- mean(boot_preds)
std_preds <- sd(boot_preds)

# Confidence interval on average grade of STAT 230
lower_bound <- mean_preds - 1.96 * std_preds
upper_bound <- mean_preds + 1.96 * std_preds


## plotting graph and error bars made from bootstrapping predictions from many different models
UB_predictions <- predictions + 1.96 * std_preds
LB_predictions <- predictions - 1.96 * std_preds


plot(actual, predictions, 
     xlab = "Actual Grades", ylab = "Predicted Grades",
     main = "Predicted vs Actual Grades with Error Bars", xlim = c(00,100), ylim = c(0,100),pch = 16)

# Add error bars
arrows(actual, LB_predictions, 
       actual, UB_predictions, 
       angle = 90, code = 3, length = 0.1, col = 1:length(predictions))

abline(0, 1, col = "red")
```


### Variable selection
This variable selection technique was taken from a **paper** with some modifications to see if removing variables improves the model. After training the model on all predictors, the two given the least importance by the gbm algorithm are removed and the model is trained again until either 2 or 1 predictors remain. For the first few courses removed it performed alright, with a rmse of ~9.5. However, as we can see the rmse never got any better and slowly increased to 10.8. 
```{r GBM recursive variable selection on prof data, eval=FALSE}

set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 8000,               # Number of trees (iterations)
  interaction.depth = 3,       # Maximum depth of trees
  shrinkage = 0.001,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 5          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# saving optimal # of trees
ntrees <- gbm.perf(gbm_model,method="cv")[1]



# Running model a second time with optimal # of trees
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)


# Make predictions on new data
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
rmse

# making df to save columns removed and rmse
model_perf <- data.frame(RMSE = rmse, col1 = NA, col2 = NA)








# creating a temp coursegrades which we will modify
temp_coursegrades <- coursegrades

# main loop to filter variables
for(loop in 1:floor((ncol(coursegrades)-3)/2)){
  
  # getting relative influence of variables
  col_inf <- matrix(relative.influence(gbm_model))
  col_inf_names <- gbm_model$var.names
  
  rel_inf <- data.frame(influence = col_inf, column = col_inf_names)
  
  rel_inf <- rel_inf[order(rel_inf$influence, decreasing = FALSE),]
  
  
  # find columns to remove
  cols_to_remove <- rel_inf[c(1,2),2]
  for(i in 1:2){
    col <- grep(cols_to_remove[i], colnames(temp_coursegrades))[1]
    temp_coursegrades <- temp_coursegrades[,-col]
  }
  
  set.seed(1234)
  n_rows <- nrow(temp_coursegrades)
  train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
  test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing
  
  
  
  # Running model first time on base params
  gbm_model <- gbm(STAT.230 ~ ., data = temp_coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)

  # saving optimal # of trees
  ntrees <- gbm.perf(gbm_model,method="cv")[1]



  # Running model a second time with optimal # of trees
  gbm_model <- gbm(STAT.230 ~ ., data = temp_coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)
  
  
  print(summary(gbm_model))
  
  # Make predictions on new data
  predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

  rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
  
  model_perf <- rbind(model_perf, c(rmse, cols_to_remove))
  
}

```

This table shows 
```{r GBM rvs, eval=TRUE}
model_perf
```



## Extreme Gradient Boosting
Both Gradient Boosting Machines and Extreme Gradient Boosting follow the sample principle of gradient boosting. However, xgboost was designed to use a more regularized model formalization to control over-fitting. While this gives it better performance, I had more luck coding GBMS. For xgboost the hyperparameter training had to be set up very differently and with new parameters such as lambda and alpha training took considerably longer. Additionally, GBMs were found to be easier to set up each time allowing for the course we predict on to be changed easily.
```{r extreme gradient boosting, eval=FALSE}
library(xgboost)

set.seed(5934)
# set.seed(555)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

## xgbm data setup
train_data <- xgb.DMatrix(as.matrix(coursegrades[train_indices,!names(coursegrades) %in% c("STAT.230")]), label = coursegrades[train_indices,"STAT.230"])
test_data <- xgb.DMatrix(as.matrix(coursegrades[test_indices,!names(coursegrades) %in% c("STAT.230")]), label = coursegrades[test_indices,"STAT.230"])



watchlist = list(train=train_data, test=test_data)

xgbm_model <- xgb.train(data=train_data, max.depth=4, eta=0.01, nthread = 2, nrounds=1000, lambda = 0, booster = "gbtree", subsample = 0.5, tree_method = "auto", watchlist=watchlist, objective = "reg:squarederror", verbose = 0, early_stopping_rounds = 10, colsample_bytree = 0.5)

# Print the summary of the trained model
summary(xgbm_model)


# Make predictions on new data
predictions <- predict(xgbm_model, test_data)

rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
rmse
```

This is the hyperparameter gridsearch for the xgboost algorithm. This is the chunk that showed that a subsample and colsample_bytree of 0.5 were best (bag.fraction and train.fraction from gbm hyperparameters).
```{r gridSearch on xGBM, eval=FALSE}
library(caret)
library(tidyverse)

searchGridSubCol <- expand.grid(subsample = c(0.5, 0.65, 0.8), 
                                colsample_bytree = c(0.5, 0.75, 1),
                                max_depth = c(1, 2),
                                lambda = c(0,1,2), 
                                alpha = c(0,1,2),
                                min_child = c(1,3),
                                eta = c(0.05, 0.01, 0.005, 0.001)
)

ntrees <- 10000

system.time(
rmseErrorsHyperparameters <- apply(searchGridSubCol, 1, function(parameterList){
  
  #Extract Parameters to test
  currentSubsampleRate <- parameterList[["subsample"]]
  currentColsampleRate <- parameterList[["colsample_bytree"]]
  currentDepth <- parameterList[["max_depth"]]
  currentEta <- parameterList[["eta"]]
  currentLambda <- parameterList[["lambda"]]
  currentAlpha <- parameterList[["alpha"]]
  currentMinChild <- parameterList[["min_child"]]
  xgboostModelCV <- xgb.cv(data =  train_data, nrounds = ntrees, nfold = 5, showsd = TRUE, 
                       metrics = "rmse", verbose = TRUE, "eval_metric" = "rmse",
                     "objective" = "reg:linear", "max.depth" = currentDepth, "eta" = currentEta,                               
                     "subsample" = currentSubsampleRate, "colsample_bytree" = currentColsampleRate
                      , print_every_n = 10, "lambda" = currentLambda, "alpha" = currentAlpha, "min_child" = currentMinChild, booster = "gbtree",
                     early_stopping_rounds = 10)
  
  xvalidationScores <- as.data.frame(xgboostModelCV$evaluation_log)
  rmse <- tail(xvalidationScores$test_rmse_mean, 1)
  trmse <- tail(xvalidationScores$train_rmse_mean,1)
  output <- return(c(rmse, trmse, currentSubsampleRate, currentColsampleRate, currentDepth, currentEta, currentMinChild))}))


# Showing hyperparameter results
xgb_hyperparams <- data.frame(t(rmseErrorsHyperparameters))
xgb_hyperparams$lambda <- searchGridSubCol$lambda
xgb_hyperparams$alpha <- searchGridSubCol$alpha

colnames(xgb_hyperparams) <- c("rmse", "trmse", "subsample", "colsample_bytree", "max_depth", "eta", "min_child", "lambda", "alpha")

xgb_hyperparams[order(xgb_hyperparams$rmse),]
```

The only error bars which could be implemented are the bootstrapped model since xgboost doesn't support establishing the distribution of the boosting process like gbm.
```{r adding bootstrapped xGBM error bars, eval=FALSE}
# bootstrapping the model x times 

n_iterations = 100  # Number of bootstrapped models
predictions = numeric(0)

for(i in 1:n_iterations){
  # Create a bootstrapped dataset
  set.seed(100+i)
  
  n_rows <- nrow(coursegrades)
  train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
  test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing
    
  # Train an XGBoost model
  train_data <- xgb.DMatrix(as.matrix(coursegrades[train_indices,!names(coursegrades) %in% c("STAT.230")]), label = coursegrades[train_indices,"STAT.230"])
  test_data <- xgb.DMatrix(as.matrix(coursegrades[test_indices,!names(coursegrades) %in% c("STAT.230")]), label = coursegrades[test_indices,"STAT.230"])

  watchlist = list(train=train_data, test=test_data)

  xgbm_model <- xgb.train(data=train_data, max.depth=4, eta=0.01, nthread = 2, nrounds=1000, lambda = 0, booster = "gbtree", subsample = 0.5, tree_method = "auto", watchlist=watchlist, objective = "reg:squarederror", verbose = 0, early_stopping_rounds = 10, colsample_bytree = 0.5)
    
  # Predict on test data
  preds <- predict(xgbm_model, test_data)
  predictions <- append(predictions, preds)
  
  sprintf("%s done", i)
}

# Convert predictions to a NumPy array
#predictions

# Calculate the mean and standard deviation of the predictions
mean_preds = mean(predictions)
std_preds = sd(predictions)


## plotting graph and error bars made from bootstrapping predictions from many different models
UB_predictions <- predictions + 1.96 * std_preds
LB_predictions <- predictions - 1.96 * std_preds


plot(actual, predictions, 
     xlab = "Actual Grades", ylab = "Predicted Grades",
     main = "Predicted vs Actual Grades with Error Bars", xlim = c(00,100), ylim = c(0,100),pch = 16)

# Add error bars
arrows(actual, LB_predictions, 
       actual, UB_predictions, 
       angle = 90, code = 3, length = 0.1, col = 1:length(predictions))

abline(0, 1, col = "red")
```



## Main Plots Used
### Predicted grades vs Actual grades
The main plot we used to show progress was plotting the predicted student grades against their actual grades. This graph immediately shows us if we're over or under predicting due to the red line. This line indicates "perfect prediction" as it follows where predicted grades = actual grades.
```{r predicted grades vs actual grades, eval=TRUE}
actual <- coursegrades[test_indices,]$COSC.221
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))

# this line represents 'perfect prediction' where actual grades = predicted grades
abline(0, 1, col = "red")

# creating and drawing line of best fit on the predicted data
line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```

### Residuals
This was added in for model checking to make sure there are no trend in residuals.
```{r residuals, eval=TRUE}
residuals <- actual - predictions
plot(predictions, residuals, xlab = "Predicted Grades", ylab = "Residuals", 
     main = "Residual Plot", pch = 16, col = "blue")
abline(h = 0, col = "red")

line.lm <- lm(residuals~predictions)
abline(line.lm, col = "green")
```

### Q-Q Plots
A q-q plot was also added in for model checking.
```{r qq plots, eval=TRUE}
qqnorm(residuals, main = "Normal Q-Q Plot of Residuals")
qqline(residuals, col = "red")
```

### Heatmap
The heatmaps were added to view the data more categorically.
This first chunk splits predictions and actual grades into intervals of 5 (eg. 50-54, 55-59...) and produces a heatmap of predicted vs actual grades.
```{r conf matrix and heat map, eval=TRUE}
library(ggplot2)
library(caret)

# Define breaks for intervals of 5
breaks <- seq(0, 100, by = 5)

# Create labels for the intervals
labels <- paste(breaks[-length(breaks)], breaks[-1], sep = "-")

actual_fac <- cut(actual, breaks = breaks, labels = labels, right = TRUE, include.lowest = TRUE)
pred_fac <- cut(predictions, breaks = breaks, labels = labels, right = TRUE, include.lowest = TRUE)

confusion_matrix <- table(actual_fac, pred_fac)


confusion_matrix <- confusionMatrix(confusion_matrix)
# Print the confusion matrix
# confusion_matrix

# Print recall for each class
rec <- confusion_matrix$byClass[, "Recall"]
prec <- confusion_matrix$byClass[, "Precision"]
f1_score <- 2*prec*rec/(prec+rec)

confusion_matrix$Freq[is.na(confusion_matrix$Freq)] <- 0

# Create the heat map
ggplot(as.data.frame(confusion_matrix$table), aes(x = actual_fac, y = pred_fac, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "red") +
  labs(x = "Actual Grades", y = "Predicted Grades", fill = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_tile(data = subset(as.data.frame(confusion_matrix$table), actual_fac == pred_fac),
            aes(fill = Freq), color = "black")
```

The second chunk splits predictions and actual grades into their letter grades and produces a heatmap of predicted vs actual grades using those splits.
```{r Heatmap on prediction letter grades, eval=TRUE}
library(caret)
library(ggplot2)

# Function to assign letter grades based on numeric ranges
assign_grades <- function(scores) {
  # Define the breaks for the numeric ranges
  breaks <- c(-Inf, 49, 54, 59, 63, 67, 71, 75, 79, 84, 89, 100)
  
  # Define the corresponding labels for each range
  labels <- c("F", "D", "C-", "C", "C+", "B-", "B", "B+", "A-", "A", "A+")
  
  # Use the cut function to assign labels based on the breaks
  grades <- cut(scores, breaks = breaks, labels = labels, right = TRUE)
  
  return(grades)
}


predictions_letter <- assign_grades(predictions)
actual_letter <- assign_grades(actual)

# displaying confusion matrix
confusionMatrix(predictions_letter, actual_letter)


confusion_matrix <- table(actual_letter, predictions_letter)
confusion_matrix <- confusionMatrix(confusion_matrix)

# Creating and displaying heat map
ggplot(as.data.frame(confusion_matrix$table), aes(x = actual_letter, y = predictions_letter, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "white", high = "red") +
  labs(x = "Actual Grades", y = "Predicted Grades", fill = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_tile(data = subset(as.data.frame(confusion_matrix$table), actual_letter == predictions_letter),
            aes(fill = Freq), color = "black")
```


