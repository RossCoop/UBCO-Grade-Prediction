---
title: "Gradient Boosted Model Prediction"
author: "Ross Cooper 54907605"
date: "2024-06-03"
output: html_document
---

# Loading in Data
```{r}
load(file="StudentGrades.RData")
head(studentgrades)
```

# Gradient Boosted Model

## Summary of the Boosting Process

1. **Initialize the model** with a constant value, typically the mean of the target values for regression:

\[ 
F_0(x) = \arg\min_{c} \sum_{i=1}^{N} L(y_i, c)
\]

2. **For \(m = 1\) to \(M\) (number of boosting iterations)**:
   - Compute the residuals \(r_i^m\)
   - For squared error loss, this simplifies to:
\[
r_i^m = y_i - F_{m-1}(x_i)
\]
   - Fit a base learner \(h_m(x)\) to the residuals.
   - Update the model via gradient decent:

\[
F_{m}(x) = F_{m-1}(x) + \nu h_m(x)
\]

3. **Final prediction** is the sum of all base learners:

\[
\hat{y} = F_M(x) = \sum_{m=1}^{M} \nu h_m(x)
\]


- What we are controlling and changing:
    - Boosting iterations (\texttt{ntrees}): variable...
    - Learning rate (\texttt{shrinkage} parameter): 0.05
    - Loss function distribution: Gaussian



## Shrinkage and Iterations

![](./GBM pics/fig3.png)

Figure 3: Out-of-sample predictive performance by number of iterations and shrinkage. Smaller values of the shrinkage parameter offer improved predictive performance, but with decreasing marginal improvement. (from vignette)

![](./GBM pics/fig4.png)

Figure 4: Out-of-sample predictive performance of four methods of selecting the optimal number of iterations. The vertical axis plots performance relative the best. The boxplots indicate relative performance across thirteen real datasets from the UCI repository. (from vignette)

The vignette recommends to use 5 or 10-fold CV if you can afford the computation time. Otherwise OOB is a good conservative choice.

## GBM Data Prep

```{r GBM data prep}
# Identifying the course of interest
COI <- "STAT.230"
coursegrades <- studentgrades[!is.na(studentgrades[,COI]),-1:-5]

# remove all grades < 60%
coursegrades <- coursegrades[coursegrades$STAT.230 >= 60, ]


# dropping columns that are very sparse (pct % are NA)
pct <- 0.95
courses <- numeric(0)
for (i in 1:ncol(coursegrades)) {
  if(is.na(coursegrades[,i]) |> sum()  > nrow(coursegrades)*pct){
    courses <- c(courses,i)
  }
}

coursegrades <- coursegrades[,-courses]


# Removing all course columns of higher years
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- column_names[column_digits <= YOI]

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]
```

## Iteration Graphs

```{r GBM cv}
library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 200,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.05,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)


gbm.perf(gbm_model, method="cv")
gbm.perf(gbm_model, method="OOB")
```

## Model Training and Prediction

```{r GBM}
set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 200,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.05,            # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)

# Running model first time on base params
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades, distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

ntrees <- gbm.perf(gbm_model,method="cv")[1]




# Running model a second time with optimal # of trees
gbm_model <- gbm(STAT.230 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = ntrees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 5)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = ntrees)

rmse <- sqrt(mean((predictions - coursegrades[test_indices,COI])^2))
rmse
```





```{r}
actual <- coursegrades[test_indices,]$STAT.230
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))
abline(0, 1, col = "red")


line.lm <- lm(predictions~actual)
abline(line.lm, col = "green")
```


