---
title: "EDA and Model Testing"
author: "Ross Cooper 54907605"
date: "2024-05-06"
output: html_document
---

```{r}
df <- read.csv("..\\UBCO-Grade-Prediction-data\\student-data.csv")
#df <- df[df$DEGR_PGM_CD == "BSC-O ",]
df_clean <- df[,c(1,2,3,4,5,6,7,8,9,10,11,12,13,15,16,17)]
#df_clean <- na.omit(df_clean)
# Do some cleaning on chr columns
df_clean$STUD_NO_ANONYMOUS <- trimws(df_clean$STUD_NO_ANONYMOUS)
df_clean$CRS_DPT_CD <- trimws(df_clean$CRS_DPT_CD)
df_clean$HDR_CRS_LTTR_GRD <- trimws(df_clean$HDR_CRS_LTTR_GRD)
df_clean$CURR_SPEC_PRIM_PGM_TYPE_1 <- trimws(df_clean$CURR_SPEC_PRIM_PGM_TYPE_1)
df_clean$CURR_SPEC_PRIM_SUBJECT_1 <- trimws(df_clean$CURR_SPEC_PRIM_SUBJECT_1)
df_clean$CURR_SPEC_PRIM_PGM_TYPE_2 <- trimws(df_clean$CURR_SPEC_PRIM_PGM_TYPE_2)
df_clean$CURR_SPEC_PRIM_SUBJECT_2 <- trimws(df_clean$CURR_SPEC_PRIM_SUBJECT_2)
df_clean$CURR_SPEC_SECN_PGM_TYPE_1 <- trimws(df_clean$CURR_SPEC_SECN_PGM_TYPE_1)
df_clean$CURR_SPEC_SECN_SUBJECT_1 <- trimws(df_clean$CURR_SPEC_SECN_SUBJECT_1)
df_clean$CURR_SPEC_SECN_PGM_TYPE_2 <- trimws(df_clean$CURR_SPEC_SECN_PGM_TYPE_2)
df_clean$CURR_SPEC_SECN_SUBJECT_2 <- trimws(df_clean$CURR_SPEC_SECN_SUBJECT_2)
df_clean$DEGR_PGM_CD <- trimws(df_clean$DEGR_PGM_CD)


# Factor grades column
grades <-
  c("A+", "A", "A-", "B+", "B", "B-", "C+", "C", "C-", "D", "F")
df_clean$HDR_CRS_LTTR_GRD <-
  factor(df_clean$HDR_CRS_LTTR_GRD, levels = grades)

# Create course code column
df_clean$COURSE_CODE <- paste(df_clean$CRS_DPT_CD, df_clean$CRS_NO, sep = ".")
df_clean <- df_clean[,-(12:13)]
df <- subset(df, HDR_CRS_PCT_GRD < 999)
df_clean <- subset(df_clean, HDR_CRS_PCT_GRD < 999)

# Removing 2023 grades (not included) [also does nothing bc ^ cleans it out]
df <- subset(df, SEC_SES_YR < 2022.5)
df_clean <- subset(df_clean, SEC_SES_YR < 2022.5)

# Subsetting only BSc students
df_bsc <- df_clean[df_clean$DEGR_PGM_CD=="BSC-O",]

df_clean
```

```{r}
# Determining how many bsc students took each course
unique(df_clean$COURSE_CODE)|>length()
courses_taken_bsc <- table(df_bsc$COURSE_CODE)


sorted_courses_taken_bsc <- sort(courses_taken_bsc, decreasing = TRUE)[1:200]#[1:450]


barplot(sorted_courses_taken_bsc)
# saving course names
course_names <- dimnames(sorted_courses_taken_bsc)[[1]]

# Subsetting on only courses which are taken the most times ("relevant courses")
df_bsc_relevant_courses <- df_bsc[df_bsc$COURSE_CODE %in% course_names, ]

# Getting a list of all student ids which took relevant courses
uniqueids <- unique(df_bsc_relevant_courses$STUD_NO_ANONYMOUS)
```



```{r}
# checking individual student ids
#df_clean[df_clean$STUD_NO_ANONYMOUS=="18AAED88A88707993EE1AF22933D1CE3",]


```

```{r}
# showing all students which never made it to second year
# need avg year of student on student id
# need to make the student df (sticky notes)

# list of all unique student IDs
uniqueids <- unique(df_bsc$STUD_NO_ANONYMOUS)

# looping through to find major(s)
unique(df_bsc$CURR_SPEC_PRIM_PGM_TYPE_1)
unique(df_bsc$CURR_SPEC_PRIM_SUBJECT_1)
unique(df_bsc$CURR_SPEC_PRIM_PGM_TYPE_2)
unique(df_bsc$CURR_SPEC_PRIM_SUBJECT_2)
unique(df_bsc$CURR_SPEC_SECN_PGM_TYPE_1)
unique(df_bsc$CURR_SPEC_SECN_SUBJECT_1)
unique(df_bsc$CURR_SPEC_SECN_PGM_TYPE_2)
unique(df_bsc$CURR_SPEC_SECN_SUBJECT_2)

subset(df_bsc, CURR_SPEC_PRIM_PGM_TYPE_1 !="" & CURR_SPEC_PRIM_PGM_TYPE_2 !="" & CURR_SPEC_SECN_PGM_TYPE_1 !="" & CURR_SPEC_SECN_PGM_TYPE_2 !="")
# looping through to find minor(s)

subset(df_bsc_relevant_courses, CURR_SPEC_PRIM_PGM_TYPE_1 =="CMJ")



# all students who only "did" 1st year and have avg of less than 30%
firstdrops <- c()
for(id in uniqueids){
  dat <- subset(df_bsc, STUD_NO_ANONYMOUS==id)
  if((mean(dat$CURR_YEAR_LEVEL) == 1) & (mean(dat$HDR_CRS_PCT_GRD) <= 30)){
    firstdrops <- append(firstdrops,id)
  }
}
firstdrops


subset(df_bsc, STUD_NO_ANONYMOUS=="A9DB35467FB793FFB63E410782268465")

#df_clean
```


```{r}
# Removing all students who dropped out horribly

```

```{r}
# finding student who've taken a course more than once
duplicate_rows <- df_bsc_relevant_courses[duplicated(df_bsc_relevant_courses[c("STUD_NO_ANONYMOUS", "COURSE_CODE")]) | duplicated(df_bsc_relevant_courses[c("STUD_NO_ANONYMOUS", "COURSE_CODE")], fromLast = TRUE), ]
duplicate_rows[c(1,15,12,13)]

# Count occurrences of each course for each student
course_counts <- table(duplicate_rows$STUD_NO_ANONYMOUS, duplicate_rows$COURSE_CODE)

# Find the maximum number of times each course was taken by any student
max_repeats <- apply(course_counts, 2, max)

# Combine course names with corresponding maximum counts
max_course_counts_df <- data.frame(COURSE_CODE = names(max_repeats), max_repeats)
#sum(max_course_counts_df$max_repeats)
```

```{r}
# Creating a new list of course column names
## eg. Math.101.1, Math.101.2...

## want a list of course names with the .1 .2 .3
# groups: Non-Repeats, Repeats
# Non-repeats
non_rep_courses <- setdiff(course_names,max_course_counts_df$COURSE_CODE)

# Repeats
# Modify course names for repeated courses
rep_courses <- c()
for (i in 1:nrow(max_course_counts_df)) {
  for (j in 1:max_course_counts_df[i,2]) {
    rep_courses <- append(rep_courses, paste0(max_course_counts_df[i,1], ".", j))
  }
}

course_names_dups <- append(non_rep_courses, rep_courses)
```


```{r}
# Finding student majors and minors
### WARNING: no 2023 data yet which may contain updates to major/minors


# looking through/ getting last row of each student
library(dplyr)

# Assuming your dataframe is called 'df'

# Group by student_id and get the last row of each group
last_rows <- df_bsc_relevant_courses %>%
  group_by(STUD_NO_ANONYMOUS) %>%
  slice(n())

# Print the result
last_rows




# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(last_rows), ncol = 4))


# Add course name columns to existing data frame
student_mm <- cbind(last_rows[,1], new_columns)
colnames(student_mm) <- c("StudentID", "Major.1", "Major.2", "Minor", "Honors")
student_mm$Honors <- FALSE

for(i in 1:nrow(last_rows)){
  # If MAJ is in column 1, major.1 is that subject
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="MAJ"){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
  }
  
  # if MAJ is in col 2, check if major.1 is occupied and place it in maj.1 or maj.2
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="MAJ" & is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="MAJ"){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  }
  
  # combined majors....
  
  # finding that minor
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="MIN"){
    student_mm[i,]$Minor <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="MIN"){
    student_mm[i,]$Minor <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  }
  
  # adding Honors as majors
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="HON" & is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
    student_mm[i,]$Honors <- TRUE
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="HON" & is.na(student_mm[i,]$Major.2)){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
    student_mm[i,]$Honors <- TRUE
  }
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="HON" & is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
    student_mm[i,]$Honors <- TRUE
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="HON" & is.na(student_mm[i,]$Major.2)){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
    student_mm[i,]$Honors <- TRUE
  }
  
  # adding combined majors
  if(last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_1=="CMJ"){
    student_mm[i,]$Major.1 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_1
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_SECN_SUBJECT_1
  } else if (last_rows[i,]$CURR_SPEC_PRIM_PGM_TYPE_2=="CMJ" & !is.na(student_mm[i,]$Major.1)){
    student_mm[i,]$Major.2 <- last_rows[i,]$CURR_SPEC_PRIM_SUBJECT_2
  }
  
}
student_mm

#student_mm[student_mm$StudentID=="48791F0E12EB466C2B84020BAB61D1A2",]
#last_rows[last_rows$STUD_NO_ANONYMOUS=="AC1CEB14281A98ACC2CD74C73F361CBF",]
#student_mm[,-1]
```


```{r}
# Making the Data Frame
 ## Columns to add: Student ID, Major, Honors, Minor, Extras, 200 most common courses

# Building df after getting the data

studentgrades <- data.frame(Student_ID = sort(uniqueids))




# Create a data frame with NA values and course column names
new_columns <- data.frame(matrix(NA, nrow = nrow(studentgrades), ncol = length(course_names_dups)))
colnames(new_columns) <- course_names_dups

# Add course name columns to existing data frame
studentgrades <- cbind(studentgrades, student_mm[,-1], new_columns)


# Saving student grades into df
for (i in 1:nrow(df_bsc_relevant_courses)){
  rep <- 0
  # finding the index of course name .1
  coursecol <- grep(df_bsc_relevant_courses[i,]$COURSE_CODE, colnames(studentgrades))[1]
  
  # Moving over from .1 to .2 columns if a grade already exists
  while(!is.na(studentgrades[studentgrades$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+rep])){
    rep = rep+1
  }
  
  # Saving student grade into studentgrades df
  studentgrades[studentgrades$Student_ID==df_bsc_relevant_courses[i,]$STUD_NO_ANONYMOUS,coursecol+rep] <- df_bsc_relevant_courses[i,]$HDR_CRS_PCT_GRD
}

studentgrades


```

```{r}
## OPTION TO REMOVE STUDENTS WITH FEW COURSES IN STUDENTGRADES
non_null_counts <- apply(studentgrades[, -1], 1, function(row) sum(!is.na(row)))

# Create a new dataframe with student ID and non-null counts
result_df <- data.frame(student_id = studentgrades[, 1], non_null_count = non_null_counts)

result_df[result_df$non_null_count >= 3,]
```

# EDA

```{r}
# Big ol graph of student grades
maths <- studentgrades[complete.cases(studentgrades$MATH.100.1, studentgrades$MATH.100.2), ]


maths.lm <- lm(maths$MATH.100.1~ maths$MATH.100.2)
summary(maths.lm)

plot(maths$MATH.100.1, maths$MATH.100.2)
abline(maths.lm)

## weird....
```

```{r}
mathstat <- studentgrades[complete.cases(studentgrades$MATH.100.1, studentgrades$STAT.230.1), ]

mathstat.lm <- lm(mathstat$MATH.100.1~ mathstat$STAT.230.1)
summary(mathstat.lm)

plot(mathstat$MATH.100.1, mathstat$STAT.230.1)
abline(mathstat.lm)

## this one is a little more normal
## could be issues with course retakers

## want to check on people who only had to take it once...
```


```{r}
edasubset <- studentgrades[complete.cases(studentgrades$MATH.100.1, studentgrades$PSYO.111.1), ]

edasubset.lm <- lm(edasubset$MATH.100.1~ edasubset$PSYO.111.1)
summary(edasubset.lm)

plot(edasubset$MATH.100.1, edasubset$PSYO.111.1)
abline(edasubset.lm)

## ... makes sense that some courses like PSYO 111 are grade boosters
```

```{r}
mathstat2 <- studentgrades[complete.cases(studentgrades$MATH.100.2, studentgrades$STAT.230.1), ]

mathstat2.lm <- lm(mathstat2$MATH.100.2~ mathstat2$STAT.230.1)
summary(mathstat2.lm)

plot(mathstat2$MATH.100.2, mathstat2$STAT.230.1)
abline(mathstat2.lm)

## want to check on people who only had to take it once...

firsttimer <- anti_join(mathstat,mathstat2)
first.lm <- lm(firsttimer$MATH.100.1~ firsttimer$STAT.230.1)
summary(first.lm)

plot(firsttimer$MATH.100.1, firsttimer$STAT.230.1, xlim = c(0,100))
abline(first.lm)

# just removed the 'low' grades (p much all fails) 
```


```{r}
# want the average grade for each course... bar plot
course_averages <- data.frame(Courses = course_names_dups, Average = rep(NA, length(course_names_dups)))

for (i in 1:nrow(course_averages)){
  course_averages[i,]$Average <- mean(studentgrades[,course_averages[i,1]],na.rm = TRUE)
}
```

```{r}
barplot(course_averages$Average, names.arg = course_averages$Courses)

subset(course_averages, Courses == "PSYO.380.4")

studentgrades[!is.na(studentgrades$PSYO.380.4),c("Student_ID","PSYO.380.4")]

course_averages
```

```{r}
plot(studentgrades[!is.na(studentgrades$MATH.100.1)&!is.na(studentgrades$MATH.101.1)&!is.na(studentgrades$STAT.230.1)&!is.na(studentgrades$DATA.101.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","DATA.101.1")])

studentgrades[!is.na(studentgrades$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")]

plot(studentgrades[!is.na(studentgrades$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")])
```
```{r}
plot(studentgrades[!is.na(studentgrades$MATH.100.1)&!is.na(studentgrades$MATH.101.1),c("MATH.100.1","MATH.101.1")])


```



```{r missForest Impute into RF}
# preparing all student grades with stat230
stat230grades <- studentgrades[!is.na(studentgrades$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")]
stat230grades

library(missForest)
library(randomForest)

n_rows <- nrow(stat230grades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,2:5])$ximp

stat230.rf <- randomForest(STAT.230.1~., data=stat230.imputed[train_indices,] )
stat230.pred <- predict(stat230.rf, newdata = stat230.imputed[test_indices,])

stat230.real <- stat230grades[test_indices,]$STAT.230.1

# MSE value to compare to other models
mse <- mean((stat230.pred - stat230.real)^2)
mse
```

```{r RF on all non-NA data}
# getting all students with grades in all courses
stat230grades.full <- studentgrades[!is.na(studentgrades$MATH.100.1)&!is.na(studentgrades$MATH.101.1)&!is.na(studentgrades$STAT.230.1)&!is.na(studentgrades$DATA.101.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","DATA.101.1")]
stat230grades.full

n_rows <- nrow(stat230grades.full)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(randomForest)

stat230.rf <- randomForest(STAT.230.1~., data=stat230grades.full[train_indices,2:5] )
stat230.pred <- predict(stat230.rf, newdata = stat230grades.full[test_indices,2:5])

stat230.real <- stat230grades.full[test_indices,2:5]$STAT.230.1

# MSE value to compare to other models
mse <- mean((stat230.pred - stat230.real)^2)
mse

## MSE of data without DATA101 is 175...
## looks like imputing data and running a randomforest is best so far
```

```{r GBM on partial-NA data}
# loading in data again
stat230grades <- studentgrades[!is.na(studentgrades$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")]
n_rows <- nrow(stat230grades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 100,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.1,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230.1 ~ ., data = stat230grades[train_indices,2:5], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = stat230grades[test_indices,2:5], n.trees = gbm_params$n.trees)

# Print the predictions
print(predictions)

mse <- mean((predictions - stat230grades[test_indices,2:5]$STAT.230.1)^2)
mse
## 175 mse... getting worse...
```

```{r GBM on non-NA data}
# loading in data again TRYING WITH FULL DATA
stat230grades <- studentgrades[!is.na(studentgrades$MATH.100.1)&!is.na(studentgrades$MATH.101.1)&!is.na(studentgrades$STAT.230.1)&!is.na(studentgrades$DATA.101.1),c("Student_ID","MATH.100.1","MATH.101.1","STAT.230.1","DATA.101.1")]
stat230grades
n_rows <- nrow(stat230grades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 100,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.1,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230.1 ~ ., data = stat230grades[train_indices,2:5], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = stat230grades[test_indices,2:5], n.trees = gbm_params$n.trees)

# Print the predictions
print(predictions)

mse <- mean((predictions - stat230grades[test_indices,2:5]$STAT.230.1)^2)
mse
## 128 mse which is a little better when you've got full data
```

```{r missForest impute into GBM}
# now, does imputing the data before GBM work better?
# preparing all student grades with stat230
stat230grades <- studentgrades[!is.na(studentgrades$STAT.230.1),c("Student_ID","MATH.100.1","MATH.101.1","DATA.101.1","STAT.230.1")]
stat230grades

library(missForest)
library(gbm)

n_rows <- nrow(stat230grades)

train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing



stat230.imputed <- missForest(stat230grades[,2:5])$ximp

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 100,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.1,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)

gbm_model <- gbm(STAT.230.1 ~ ., data = stat230.imputed[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = stat230.imputed[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
print(predictions)

mse <- mean((predictions - stat230.imputed[test_indices,]$STAT.230.1)^2)
mse
## 109 mse.. better still
```


```{r Pipeline for taking a course grade to predict on}
# takes course (eg. COSC 322)
# finds all predictors to predict on... run gbm.. take those with top influence?



stat230grades <- studentgrades[!is.na(studentgrades$STAT.230.1),-1:-5]




grade_counts <- colSums(!is.na(stat230grades))
# Subset columns with less than 10 grades
sparse_cols <- names(grade_counts[grade_counts < 10])
# Subset the dataframe to include only columns with more than 10 grades
stat230grades <- stat230grades[, -which(names(stat230grades) %in% sparse_cols)]





n_rows <- nrow(stat230grades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 100,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.1,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)

# ****CURRENT ERROR****** currently is stopping as some columns only contain few grades (only existing in test set) no bueno and cant train.
# TODO: remove those cols with few data
gbm_model <- gbm(STAT.230.1 ~ ., data = stat230grades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = stat230grades[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
print(predictions)

mse <- mean((predictions - stat230grades[test_indices,]$STAT.230.1)^2)
mse
# 112.9313 eh
```



```{r}
### This first removes all courses which are in higher levels (eg. if predicting on STAT.230, 300's and 400's are removed)
### Then courses which have < 10 grades in them are removed (this is still by chance and may break) temp fix for NA courses in training set
### GBM used on remaining courses to predict for the course of interest
set.seed(5934)

# time to remove all future year courses from prediction
COI <- "COSC.322"
coursegrades <- studentgrades[!is.na(studentgrades[,COI]),-1:-5]

# figuring out the year level of each course...
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- column_names[column_digits <= YOI]

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# extras/ to do next
grade_counts <- colSums(!is.na(coursegrades))
# Subset columns with less than 10 grades
sparse_cols <- names(grade_counts[grade_counts <= 5])

# Need to exclude future attempts at the course, but not past
## eg. PSYO.380.1 shouldn't use 380.2 as a predictor, but 380.2 can use 380.1
if(length(strsplit(COI, split = ".", fixed = TRUE)[[1]]) == 3){
  cname <- strsplit(COI, split = ".", fixed = TRUE)[[1]][1]
  cnum <- strsplit(COI, split = ".", fixed = TRUE)[[1]][2]
  crep <- as.integer(strsplit(COI, split = ".", fixed = TRUE)[[1]][3])
  # remove all future instances of course (if they exist)
  for(i in 1:length(column_names)){
    if(strsplit(column_names[i], split = ".", fixed = TRUE)[[1]][1] == cname & strsplit(column_names[i], split = ".", fixed = TRUE)[[1]][2] == cnum & as.integer(strsplit(column_names[i], split = ".", fixed = TRUE)[[1]][3]) > crep){
      sparse_cols <- append(sparse_cols, column_names[i])
    }
  }
}

# Subset the dataframe to include only courses used to predict on
coursegrades <- coursegrades[, -which(names(coursegrades) %in% sparse_cols)]
```

```{r}
set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 100,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.1,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)



gbm_model <- gbm(COSC.322 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
predictions

mse <- mean((predictions - coursegrades[test_indices,COI])^2)
mse

```
```{r}
## removes all courses of equal or higher level before prediction
set.seed(5934)
# time to remove all future year courses from prediction
COI <- "COSC.322"
coursegrades <- studentgrades[!is.na(studentgrades[,COI]),-1:-5]

# figuring out the year level of each course...
YOI <- as.integer(substr(strsplit(COI, split = ".", fixed = TRUE)[[1]][2],1,1))

column_names <- colnames(coursegrades)

# Extract digits after the period in column names
column_digits <- as.integer(sub("^[^.]+\\.([0-9]).*", "\\1", column_names))

# Find columns with digits not matching the selected course year
cols_to_keep <- column_names[column_digits < YOI]
cols_to_keep <- append(cols_to_keep, COI)

# All columns in the same year or below
coursegrades <- coursegrades[, colnames(coursegrades) %in% cols_to_keep]

# extras/ to do next
grade_counts <- colSums(!is.na(coursegrades))
# Subset columns with less than 10 grades
sparse_cols <- names(grade_counts[grade_counts <= 5])


# Subset the dataframe to include only courses used to predict on
coursegrades <- coursegrades[, -which(names(coursegrades) %in% sparse_cols)]
```


```{r}
set.seed(5934)
n_rows <- nrow(coursegrades)
train_indices <- sample(1:n_rows, 0.8 * n_rows)  # 80% of rows for training
test_indices <- setdiff(1:n_rows, train_indices) # remaining rows for testing

library(gbm)

gbm_params <- list(
  distribution = "gaussian",  # Specify the distribution for regression
  n.trees = 100,               # Number of trees (iterations)
  interaction.depth = 4,       # Maximum depth of trees
  shrinkage = 0.1,             # Learning rate (shrinkage)
  bag.fraction = 0.5,          # Fraction of training data used for each tree
  train.fraction = 1.0,        # Fraction of training data used for training (1.0 for full dataset)
  n.minobsinnode = 10          # Minimum number of observations in terminal nodes
)



gbm_model <- gbm(COSC.322 ~ ., data = coursegrades[train_indices,], distribution = gbm_params$distribution,
                 n.trees = gbm_params$n.trees, interaction.depth = gbm_params$interaction.depth,
                 shrinkage = gbm_params$shrinkage, bag.fraction = gbm_params$bag.fraction,
                 train.fraction = gbm_params$train.fraction, n.minobsinnode = gbm_params$n.minobsinnode, cv.folds = 10)

# Print the summary of the trained model
print(summary(gbm_model))

# Make predictions on new data
# Assuming 'new_data' is your new dataframe for prediction
predictions <- predict(gbm_model, newdata = coursegrades[test_indices,], n.trees = gbm_params$n.trees)

# Print the predictions
predictions

mse <- mean((predictions - coursegrades[test_indices,COI])^2)
mse

```



```{r}
actual <- coursegrades[test_indices,]$COSC.322
plot(actual, predictions, xlab = "Actual Grades", ylab = "Predicted Grades", 
     main = "Predicted vs Actual Grades", pch = 16, col = "blue", xlim = c(0,100), ylim = c(0,100))
abline(0, 1, col = "red")


line.lm <- lm(actual~predictions)
abline(line.lm, col = "green")
```

```{r}
residuals <- actual - predictions
plot(predictions, residuals, xlab = "Predicted Grades", ylab = "Residuals", 
     main = "Residual Plot", pch = 16, col = "blue")
abline(h = 0, col = "red")

line.lm <- lm(residuals~predictions)
abline(line.lm, col = "green")
```

```{r}
qqnorm(residuals, main = "Normal Q-Q Plot of Residuals")
qqline(residuals, col = "red")
```

